% LaTeX source for ``การเรียนรู้ของเครื่องสำหรับเคมีควอนตัม (Machine Learning for Quantum Chemistry)''
% Copyright (c) 2022 รังสิมันต์ เกษแก้ว (Rangsiman Ketkaew).

% License: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)
% https://creativecommons.org/licenses/by-nc-nd/4.0/

\chapter{การทำนายคุณสมบัติของโมเลกุล}
\label{ch:predict_molprop}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig/ml_pred_chem.png}
    \caption{การสำรวจคุณสมบัติเชิงโมเลกุลในปริภูมิเชิงเคมีแบบ Local และ Global 
    (เครดิตภาพ: \textit{J. Chem. Phys.} 2021, 154, 230903 \autocite{westermayr2021b})}
    \label{fig:ml_pred_chem}
\end{figure}

ในบทนี้เราจะมาศึกษาการทำนายสมบัติของโมเลกุลด้วย ML ซึ่งถือได้ว่าเป็นหัวใจสำคัญของส่วนที่สองของหนังสือเล่มนี้เลยก็ว่าได้ โดยผู้อ่านจะได้%
เรียนรู้และทำความเข้าใจลำดับขั้นตอนในการใช้โมเดล ML ในการทำนายคุณสมบัติเชิงอิเล็กทรอนิกส์ทางควอนตัมต่าง ๆ ที่ได้ศึกษาไปแล้วในบทที่ 
\ref{ch:el_prop} ซึ่งเป้าหมายหลักของการทำวิจัยในสาขานี้ก็คือการศึกษาและพัฒนา Representation และโมเดล ML ที่สามารถทำนาย%
คุณสมบัติเชิงควอนตัมของโมเลกุลที่ต้องการได้อย่างแม่นยำและเทียบเคียงได้กับผลจากการคำนวณด้วยวิธีเคมีควอนตัมแบบมาตรฐาน เช่น วิธี DFT 
และวิธีเชิงฟังก์ชันคลื่น\autocite{ceriotti2018} นอกจากนี้ผู้อ่านจะได้ศึกษาตัวอย่างโค้ดที่สามารถนำไปใช้งานในการทำนายได้จริงซึ่งผู้เขียน%
เชื่อว่าการศึกษาทฤษฎีควบคู่ไปพร้อมกับการเขียนโปรแกรมนั้นจะช่วยทำให้เข้าใจทฤษฎีได้อย่างถูกต้องและรวดเร็วเพราะการเขียนโปรแกรมช่วยให้เรา%
คิดอย่างเป็นระบบและเข้าใจสิ่งที่ศึกษาอย่างเป็นขั้นเป็นตอน
\idxboth{การทำนายคุณสมบัติของโมเลกุล}{Molecular Property Prediction}

%--------------------------
\section{แนวทางการปฏิบัติ}
\label{sec:pred_best_prac}
%--------------------------

แนวทางปฏิบัติ (Best Practice) หรือลำดับขั้นตอนสำหรับการนำ ML มาประยุกต์กับเคมีควอนตัมสามารถแบ่งออกเป็น 6 ขั้นตอนง่าย ๆ ได้ดังนี้
\idxen{Best Practice}

\begin{enumerate}
    \item วิเคราะห์ชุดข้อมูลดิบและทำความสะอาดข้อมูล (Data Analysis และ Data Cleaning)
    
    \item เลือก Representation/Descriptor ที่จะนำมาคำนวณ Feature 
    
    \item เลือกอัลกอริทึม ML ที่เหมาะสมกับโจทย์หรือชุดข้อมูล
    
    \item ฝึกสอนโมเดลและทำนายคำตอบ
    
    \item ศึกษาผลกระทบจากการเปลี่ยน Hyperparameter และทำ Validation ต่อความถูกต้องในการทำนายเพื่อประเมินประสิทธิภาพของโมเดล
    
    \item สรุปปัจจัยและพารามิเตอร์ที่ทำให้โมเดลมีประสิทธิภาพดีที่สุดและวิเคราะห์ผลการทำนาย
\end{enumerate}

ขั้นตอนแรกของการนำ ML มาประยุกต์กับเคมีควอนตัมก็คือการเตรียมข้อมูลดิบ (Raw Data) นั่นก็คือข้อมูลทางเคมีเบื้องต้นของโมเลกุลที่เรามี 
โดยส่วนใหญ่แล้วนักเคมีเชิงคำนวณมักจะเริ่มต้นด้วยการเตรียม Cartesian Coordinates ของชุดโมเลกุลที่ต้องการศึกษา เช่น โมเลกุลอินทรีย์
สารประกอบโลหะ ฯลฯ เมื่อเรามีชุดข้อมูลแล้วสิ่งที่เราควรทำต่อไปก็คือการวิเคราะห์ข้อมูลแบบเชิงลึก (Exploratory Data Analysis หรือ EDA) 
ซึ่งในขั้นตอนนี้เราควรจะต้องทำความสะอาดข้อมูลดิบด้วย เช่น การนำค่าผิดปกติออกไป การแปลงพารามิเตอร์บางค่าให้อยู่ในรูปแบบที่เหมาะสม 
โดยเราสามารถใช้เทคนิค Unsupervised ML เช่น Elliptic Envelope, Isolation Forest, และ Local Outlier Factor (LOF)
ในการตรวจหาค่าผิดปกติได้
\idxen{Best Practice!Data Analysis}
\idxen{Best Practice!Data Cleaning}

ลำดับถัดมาคือเราจะต้องเลือก Representation หรือ Descriptor ที่เราต้องการนำมาคำนวณมาคุณสมบัติต่าง ๆ ของโมเลกุล 
ซึ่งเรามักจะได้มาจากการคำนวณด้วยวิธีแบบดั้งเดิม โดยการคำนวณ Representation นั้นก็จะมีให้เลือกมากมาย ขึ้นอยู่กับความสอดคล้องของอินพุต 
(Feature ที่เราคำนวณ) กับเอาต์พุตที่เราต้องการจะทำนาย ซึ่งขั้นตอนนี้จะเป็นการสร้าง Feature Vector สำหรับการฝึกโมเดล ML นั่นเอง 
เมื่อเราได้ชุดข้อมูลที่มี Input Feature แล้ว อาจจะมีขั้นตอนที่เพิ่มเข้ามาเพื่อช่วยให้เราเข้าใจชุดข้อมูลได้มากขึ้น เช่น เราอาจจะใช้วิธีทางสถิติเข้า%
มาช่วยคำนวณค่าทางสถิติของชุดข้อมูลก่อนนำไปฝึกสอนโมเดล เช่น ค่าเฉลี่ย (Mean), ค่าเบี่ยงเบน (Deviation), การแจกแจงความถี่, 
(Frequency), ความแปรปรวน (Variance), และสหสัมพันธ์ของเพียร์สัน (Pearson Correlation) ซึ่งค่าทางสถิติเหล่านี้จะช่วยให้เข้าใจการ%
กระจายตัวในชุดข้อมูลรวมไปถึงความสำคัญ (Importance) ของ Feature แต่ละตัวในชุดข้อมูล ซึ่งนำไปสู่การตัดสินใจและวิเคราะห์ว่า Feature 
ตัวไหนที่น่าจะมีผลต่อประสิทธิภาพของโมเดลเรามากที่สุด 

เมื่อเราได้ชุดข้อมูลที่มีความเหมาะสมแล้ว ขั้นตอนต่อมาคือการเลือกเทคนิค ML ที่เราต้องการจะใช้สำหรับฝึกสอน ข้อแนะนำก็คือในช่วงเริ่มต้นเราอาจ%
จะยังไม่ต้องไปใช้เทคนิคที่ซับซ้อนหรืออลังการมากก็ได้ เพราะการที่เราใช้เทคนิคที่ซับซ้อนหรือมีความสิ้นเปลืองสูงตั้งแต่แรกนั้นอาจจะไม่ได้การันตีว่า%
เราจะได้โมเดลที่ดีเสมอไปและยังเสียเวลาอีกด้วย ดังนั้นการเลือกใช้เทคนิคง่าย ๆ เช่น Ridge Regression ในช่วงเริ่มต้นก็อาจจะทำให้เรามีโมเดล 
MLP ที่มีประสิทธิภาพมาก ๆ แล้วก็ได้ นอกจากนี้ผู้เขียนมักจะพบเห็นผู้ที่เพิ่งเริ่มศึกษา ML หลาย ๆ คนที่เริ่มฝึกสอนโมเดลด้วย Deep Neural 
Network โดยการใช้เทคนิคขั้นสูงกับข้อมูลที่มีความเรียบง่ายซึ่งตรงจุดนี้บางครั้งมันก็มีความไม่เหมาะสมระหว่างเทคนิคและข้อมูลที่เรามี ซึ่งการทำ%
แบบนี้เราอาจจะเรียกว่าขี่ช้างจับตั๊กแตน อย่างไรก็ตามประเด็นการเลือกใช้เทคนิค ML นี้เป็นเพียงความเห็นของผู้เขียนซึ่งท้ายที่สุดแล้วก็ขึ้นอยู่กับ%
วิจารณญานของแต่ละคนครับ

เมื่อเราเลือกเทคนิค ML ได้แล้ว ขั้นตอนต่อมาก็คือการสร้างโมเดลและฝึกสอนกับ Training Set โดยในขั้นตอนนี้เราอาจจะลองสร้างหลาย ๆ 
โมเดลและทำการปรับ Hyperparameter ไปด้วยก็ได้ (ควรเปลี่ยนค่าอย่างเป็นระบบและให้สอดคล้องกัน) นอกจากนี้เราอาจจะทำ Validation 
เพิ่มด้วยก็ได้เพื่อเป็นการทดสอบความสามารถของเทคนิค ML ที่เราได้เลือกมาใช้ว่ามีประสิทธิภาพอย่างไร เมื่อเราได้โมเดลที่ถูกฝึกสอนมาแล้ว 
ลำดับต่อมาก็คือการทำนายหรือพยากรณ์คำตอบนั่นเอง โดยเราควรจะต้องมาวิเคราะห์ถึงปัจจัยที่ส่งผลต่อการทำนาย พยายามหาความเชื่อมโยงระหว่าง 
Feature ที่เลือกใช้, เทคนิค ML และ Hyperparameters ต่าง ๆ ที่เราได้กำหนดและลองปรับเปลี่ยนค่าในระหว่างการฝึกสอนโมเดล 
เมื่อเราได้โมเดลที่ถูกฝึกสอนมาอย่างดีและมีประสิทธิภาพที่อยู่ในเกณฑ์ที่ยอมรับได้แล้วนั้น เราก็จะมีโมเดลที่พร้อมจะไปใช้งานจริงครับ

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig/ml_workflow.png}
    \caption{แนวทางและขั้นตอนการสร้างโมเดลปัญญาประดิษฐ์แบบที่ 1 (เครดิตภาพ: \url{https://learnbyinsight.com})}
    \label{fig:ml_workflow}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig/ml_pipeline.png}
    \caption{แนวทางและขั้นตอนการสร้างโมเดลปัญญาประดิษฐ์แบบที่ 2 (เครดิตภาพ: \url{https://cds.cern.ch})}
    \label{fig:ml_pipeline}
\end{figure}

%--------------------------
\section{การเลือกโมเดลที่เหมาะสม}
\label{sec:choose_ml_model}
\idxen{Model Selection}
%--------------------------

ปัจจัยที่เราควรพิจารณาในการเลือกอัลกอริทึม ML สำหรับทำการฝึกสอนโมเดลมีดังนี้

\begin{itemize}[topsep=0pt]
    \item การตีความของอัลกอริทึม (Interpretability): เมื่อเราพูดถึง Interpretability ของอัลกอริทึม ML หมายความว่าเรากำลัง%
    พูดถึงความสามารถของอัลกอริทึมในการอธิบายการทำนายผล ซึ่งเราเรียกอัลกอริทึมที่ขาดความสามารถในการอธิบายดังกล่าวว่ากล่องดำ 
    (Black-box)

    อัลกอริทึมอย่างเช่น KNN มี Interpretability ที่สูงมากผ่านการทำ Feature Importance และอัลกอริทึมอย่างเช่นโมเดลเชิงเส้นมี 
    Interpretability สูงผ่านการคำนวณน้ำหนัก (Weights) ของแต่ละ Feature 
    
    \item จำนวนของข้อมูลในชุดข้อมูลและจำนวน Features: ขนาดของชุดข้อมูลมีผลอย่างมากต่อการเลือกอัลกอริทึม ML เช่น อัลกอริทึม 
    Neural Network นั้นทำงานได้ดีกับชุดข้อมูลที่มีขนาดใหญ่และมีปริมาณ Feature ที่เยอะ
    
    \item ชนิดของข้อมูล (Data Format): ปกติแล้วชนิดของข้อมูลนั้นมีหลากหลายรูปแบบ โดยชนิดของข้อมูลส่วนใหญ่จะเป็นแบบหมวดหมู่
    (Categorical) และแบบตัวเลข (Numerical) ซึ่งบางชุดข้อมูลอาจจะมีแค่หมวดหมู่หรืออาจจะมีแค่ตัวเลข และอาจจะมีทั้งสองแบบก็ได้
    
    \item ความเป็นเส้นตรงของข้อมูล (Linearity of Data): การเข้าใจความเป็นเชิงเส้นของข้อมูลนั้นสำคัญมากเพราะว่าจะเป็นการช่วยให้%
    เราสามารถวิเคราะห์ขอบเขตการตัดสินใจและเส้น Regression Line ตัวอย่างเช่นถ้าข้อมูลสามารถถูกแยกได้แบบเชิงเส้นหรือเส้นตรงเราก็%
    สามารถใช้อัลกอริทึมแบบเชิงเส้นอย่างเช่น SVM, Linear Regression หรือ Logistic Regression ได้ แต่ถ้าโมเดลมีความไม่เป็นเชิง%
    เราก็ควรใช้อัลกอริทึมอย่างเช่น Neural Network
    
    \item ระยะเวลาในการฝึกสอนโมเดล (Training Time): ระยะเวลาในการฝึกสอนโมเดลของแต่ละอัลกอริทึมนั้นต่าง อัลกอริทึมพื้นฐานเช่น 
    KNN และ Logistic Regression นั้นใช้ระยะเวลาไม่นานมากเมื่อเทียบกับอัลกอริทึมอื่น ส่วนอัลกอริทึมที่มีความซับซ้อนเช่น Neural
    Network นั้นมักจะใช้เวลานานในการฝึกสอน นอกจากนี้ยังมีอัลกอริทึมบางประเภทที่ระยะเวลาในการฝึกสอนนั้นขึ้นอยู่กับจำนวนของ CPU Cores 
    ที่ใช้ในการรัน เช่น Random Forest 
    
    \item ระยะเวลาในการทำนาย (Prediction Time): นอกเหนือจากระยะเวลาในการฝึกสอนเราก็ควรคำนึงถึงระยะเวลาที่ใช้ในการทำนายด้วย 
    โดยอัลกอริทึมอย่างเช่น Linear Regression, Logistic Regression และ Neural Network บางประเภทนั้นสามารถทำนายได้อย่าง%
    รวดเร็ว อย่างไรก็ตามอัลกอริทึมอย่างเช่น KNN หรือการทำ Ensemble Model นั้นใช้เวลาเยอะกว่ามากในการทำนาย
    
    \item หน่วยความจำที่ต้องใช้ (Memory Requirements): ถ้าหากชุดข้อมูลของเรามีขนาดใหญ่ก็จะทำให้พารามิเตอร์หรือตัวแปรที่ถูกสร้าง%
    ขึ้นและถูกคำนวณในระหว่างการฝึกสอนโมเดลนั้นเยอะตามไปด้วย ส่งผลให้ปริมาณของหน่วยความจำ (Memory) นั้นต้องเพียงพอสำหรับการ%
    ฝึกสอนโมเดล ดังนั้นถ้าหากเรามีข้อมูลที่มีขนาดใหญ่ การเลือกอัลกอริทึม ML ที่เหมาะสมและไม่ซับซ้อนมากก็จะทำให้ไม่มีปัญหาเกี่ยวกับ 
    Memory
\end{itemize}

นอกจากนี้แล้วผู้อ่านขออธิบายเพิ่มเติมในส่วนของการเลือกเคอร์เนล (Kernel) สำหรับการทำ Regression โดยเคอร์เนลที่มักถูกใช้นั้นมีดังต่อไปนี้
\idxboth{เคอร์เนล}{Kernel}

\fbox{%
\begin{minipage}{0.9\linewidth}
    \begin{align*}  
        &\text{Linear Kernel}: &&K(x_{i}, x_{j}) = x_{i} \cdot x_{j} \\[0.5ex]
        &\text{Polynomial Kernel}: &&K(x_{i}, x_{j}; a, b) = (x_{i} \cdot x_{j} + a)^b \\[0.5ex]
        &\text{Gaussian Kernel}: &&K(x_{i}, x_{j}; w, \sigma) = \exp \left(-\frac{|x_{i}-x_{j}|^2}{2\sigma^2} 
        \right) \\[1.5ex]
        &\text{Laplacian Kernel}: &&K(x_{i}, x_{j}; w, \gamma) = \exp \left(-{|x_{i}-x_{j}|}\right) \\[0.5ex]
    \end{align*}
\end{minipage}}
\idxboth{เคอร์เนล!เชิงเส้น}{Kernel!Linear}
\idxboth{เคอร์เนล!พหุนาม}{Kernel!Polynomial}
\idxboth{เคอร์เนล!เกาส์เซียน}{Kernel!Gaussian}
\idxboth{เคอร์เนล!ลาปลาเซียน}{Kernel!Laplacian}

การเลือก Kernel ที่เหมาะสมนั้นจริง ๆ แล้วขึ้นกับอัลกอริทึมของ ML ที่เราจะเลือกใช้ด้วย โดย Kernel ที่สามารถนำไปใช้แล้วทำให้เกิดการเรียนรู้
Regression ได้อย่างดีเยี่ยมก็คือ Gaussian Kernel นั่นก็เพราะว่ามีคุณสมบัติ อย่างเช่น ความสมมาตรและความต่อเนื่องตลอดช่วงของฟังก์ชัน

%--------------------------
\section{การทำนายพลังงานรวมของโมเลกุล}
\label{sec:pred_tot_ener}
\idxth{การทำนายคุณสมบัติของโมเลกุล!พลังงานรวมของโมเลกุล}
\idxen{Molecular Property Prediction!Total Energy}
%--------------------------

การคำนวณพลังงานรวมเชิงอิเล็กทรอนิกส์ของโมเลกุลถือได้ว่าเป็นหนึ่งในการคำนวณพื้นฐานที่สุดในการศึกษาความเสถียรของโมเลกุลเลยก็ว่าได้
โดยเราจะมาดูการเขียนโค้ดสำหรับการทำนายพลังงานของโมเลกุลจากชุดข้อมูล QM9 โดยใช้ Feature ที่เป็น Coulomb Matrix (CM) 
ซึ่งเราได้ดูรายละเอียดการคำนวณ CM รวมถึงการวิเคราะห์ไปแล้วในหัวข้อที่ \ref{sec:dataset_analysis} โดยโมเดล ML ที่ผู้เขียนเลือกคือ
Molecular Kernel สำหรับการทำ Regression ซึ่งสามารถเรียกใช้ได้จากไลบรารี่ QML

ไอเดียของ Molecular Kernel ก็คือเริ่มต้นด้วยการกำหนด Kernel ขึ้นมาก่อน ดังนี้

\begin{equation}\label{eq:mol_kernel}
    \alpha = (\bm{K} + \lambda\bm{I} ) ^{-1} \bm{y}
\end{equation}

\noindent เป้าหมายของเราก็คือการหาค่าของพารามิเตอร์ $\alpha$ ซึ่งเป็นสัมประสิทธิ์ที่เหมาะสมที่สุด (Best Fit for Regression) 
นั่นหมายความว่าเราจะต้องทำการแก้สมการที่ \ref{eq:mol_kernel} ซึ่งสามารถทำได้โดยใช้ Cholesky Decomposition%
\footnote{การแยกส่วนประกอบโชเลสกี (Cholesky Decomposition เป็นวิธีการแยกเมทริกซ์ของเมทริกซ์สมมาตรที่เป็นบวกแน่นอน (Symmetric 
Positive-definite Matrix) ไปเป็นเมทริกซ์สามเหลี่ยมล่าง (Lower Triangular Matrix, $L)$ และเมทริกซ์สลับเปลี่ยนของเมทริกซ์สาม%
เหลี่ยมล่าง $(L^{T})$}

เราเริ่มต้นเขียนโค้ดสำหรับแบ่งชุดข้อมูลออกเป็นชุดข้อมูลสำหรับการฝึกสอนและการทดสอบตามลำดับ
\begin{lstlisting}[style=MyPython]
from sklearn.model_selection import train_test_split

X_cm_train, X_cm_test, Y_cm_train, Y_cm_test = train_test_split(cm, target, test_size=0.2, random_state=42)
\end{lstlisting}

\noindent ตามด้วยการกำหนด Kernel ซึ่งเราจะใช้ Gaussian Kernel 

\begin{equation}\label{eq:gaussian_kernel}
    K(x_{i}, x_{j}) = \exp \left( -\frac{||x_{i}-x_{j}||^2}{2\sigma^2} \right)
\end{equation}

\noindent พร้อมกับสร้างลิสต์ของ $\sigma$ หลาย ๆ ค่า ซึ่งเราจะทำการหาค่าที่เหมาะสมที่สุดโดยการคำนวณระยะห่างแบบคู่ (Pairwise) 
ระหว่าง CM เพื่อเพิ่มความเร็วในการคำนวณเพราะว่าเราจะทำการแก้ Kernel หลาย ๆ อันพร้อมกับเปลี่ยนค่า $\sigma$ หลาย ๆ ค่าไปพร้อม ๆ กัน

\begin{lstlisting}[style=MyPython]
# Generates a list of different sigma values
sigmas = np.arange(100,5000,500) 
test_maes = []

# Compute the pairwise distances between cm representations
dm_train_train = sklearn.metrics.pairwise_distances(
    X_cm_train, 
    X_cm_train, 
    n_jobs=-1
    )
dm_train_test = sklearn.metrics.pairwise_distances(
    X_cm_train, 
    X_cm_test, 
    n_jobs=-1
    )
\end{lstlisting}

\vspace{1em}
\noindent หลังจากนั้นจึงเริ่มทำการหาค่า $\sigma$ โดยใช้ Loop

\begin{lstlisting}[style=MyPython]
for sigma in sigmas:
    # Step 1
    K_cm = np.exp( - dm_train_train ** 2 / (2 * sigma ** 2)) 
    # Step 2
    K_cm[np.diag_indices_from(K_cm)] += 1e-8
    # Step 3
    alpha_cm = cho_solve(K_cm, Y_cm_train)
    # Step 4
    K_cm_test = np.exp( - dm_train_test ** 2 / (2 * sigma ** 2))
    # Step 5
    Y_cm_predicted = np.dot(K_cm_test.T, alpha_cm)
    # Step 6
    test_MAE = np.mean(np.abs(Y_cm_predicted - Y_cm_test))
    test_maes.append(test_MAE)
\end{lstlisting}

\vspace{1em}
โดยสิ่งที่เกิดขึ้นภายใน Loop ของโค้ดด้านบนมีขั้นตอนดังนี้

\begin{enumerate}[noitemsep]
    \item เราเริ่มด้วยการคำนวณ Kernel ซึ่งถูกกำหนดด้วยสมการ \ref{eq:mol_kernel}
    
    \item เพิ่มค่า $\lambda$ ที่มีค่าน้อย ๆ เข้าไปในสมาชิกแนวทแยงของ Kernel Matrix
    
    \item แก้สมการโดยใช้ Cholesky Decomposition
    
    \item คำนวณ Kernel Matrix ระหว่างชุดข้อมูลที่ใช้ฝึกสอนกับชุดที่ใช้ทดสอบโดยใช้ค่า $\sigma$ เดียวกัน
    
    \item ทำการพยากรณ์หรือทำนายค่าพลังงานภายในของโมเลกุล
    
    \item คำนวณค่าความคลาดเคลื่อน Mean Absolute Error (MAE)
\end{enumerate}

เมื่อทำ Regression เสร็จแล้ว เราสามารถพล็อตกราฟเพื่อดูความสัมพันธ์ค่า $\sigma$ ของ Kernel กับค่าความคลาดเคลื่อน MAE ได้ดังนี้

\begin{lstlisting}[style=MyPython]
import matplotlib.pyplot as plt

fig, ax = plt.subplots()

ax.plot(sigmas, test_maes)
ax.set_ylabel('MAE kcal/mol', fontsize=15)
ax.set_xlabel('Kernel width $\sigma$', fontsize=15)
ax.tick_params(axis='both', which='major', labelsize=12)

plt.show()
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/qm9_cm_mae_sigma.png}
    \caption{ค่าความคลาดเคลื่อน MAE กับค่าความกว้างของ Kernel $(\sigma)$}
    \label{fig:qm9_cm_kernel_mae}
\end{figure}

นอกจากนี้เรายังสามารถทำการวิเคราะห์เพิ่มเติมได้ เช่น แสดงค่า $\sigma$ ที่เหมาะสมที่สุด (ให้ MAE น้อยที่สุด)

\begin{lstlisting}[style=MyPython]
best_sigma = sigmas[np.argmin(test_maes)]
print(best_sigma)

# Output
3100
\end{lstlisting}

\vspace{1em}
\noindent แล้วนำค่า $\sigma$ ที่เหมาะสมที่สุดนี้ไปใช้ในการคำนวณ Gaussian Kernel ต่อไป ดังนี้

\begin{lstlisting}[style=MyPython]
# Create Gaussian Kernel
K_cm = gaussian_kernel(X_cm_train, X_cm_train, sigma)

# Add a small lambda to the diagonal of the kernel matrix
K_cm[np.diag_indices_from(K_cm)] += 1e-8

# Use the built-in Cholesky-decomposition to solve
alpha_cm = cho_solve(K_cm, Y_cm_train)
print(alpha_cm)

# Output
[-1.72214025e+09 -1.37779221e+09 -6.40402006e+08 ...  2.42741968e+09  5.94869282e+08
 -1.19747799e+09]
\end{lstlisting}

\vspace{1em}
\noindent แล้วคำนวณต่อ Kernel ระหว่างชุดข้อมูลฝึกสอนกับชุดข้อมูลทดสอบได้โดยใช้ค่า $\sigma$ เดียวกัน

\begin{lstlisting}[style=MyPython]
# Calculate a kernel matrix between test and training data, using the same sigma
K_cm_test = gaussian_kernel(X_cm_test, X_cm_train, sigma)

# Make the predictions
Y_cm_predicted = np.dot(K_cm_test, alpha_cm)

# Calculate mean-absolute-error (MAE), the units are Hartree
print('MAE: ', np.mean(np.abs(Y_cm_predicted - Y_cm_test)), 'kcal/mol')

# Output
MAE:  25.908959327933474 kcal/mol
\end{lstlisting}

\vspace{1em}
ขั้นตอนสุดท้ายคือการพล็อต Correlation ระหว่างค่าอ้างอิง (Reference) กับค่าที่ได้จากการทำนาย (Prediction) และแสดง Histogram 
ของความคลาดเคลื่อนเพื่อตรวจสอบประสิทธิภาพของโมเดล

\begin{lstlisting}[style=MyPython]
import matplotlib.pyplot as plt

fig, axes = plt.subplots(ncols=1, nrows=2, figsize=[6,8])

ax = axes[0]
ax.scatter(Y_cm_test, Y_cm_predicted, s=16)
ax.set_ylabel(
    'CM-KRR internal energies at 0 K \n prediction [kcal/mol]', 
    fontsize=15
    )
ax.set_xlabel(
    'Internal energies at 0 K [kcal/mol]', 
    fontsize=15
    )

ax = axes[1]
ax.hist(Y_cm_test - Y_cm_predicted, bins=50, range=[-100,100], density=True)
ax.set_ylabel('Histogram density', fontsize=15)
ax.set_xlabel(
    'CM-KRR internal energies at 0 K errors [kcal/mol]', 
    fontsize=15
    )

plt.tight_layout()
plt.show()
\end{lstlisting}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/qm9_cm_kernel_corr.png}
    \caption{ซ้าย: Correlation ของค่าพลังงานภายในของโมเลกุลที่เป็นค่าอ้างอิงและค่าที่ได้จากการทำนาย, ขวา: Histrogram Density 
    ของค่าคลาดเคลื่อน}
    \label{fig:qm9_cm_kernel_corr}
\end{figure}

%--------------------------
\section{การทำนายพื้นผิวพลังงานศักย์}
\label{sec:pred_pot_ener}
\idxth{การทำนายคุณสมบัติของโมเลกุล!พื้นผิวพลังงานศักย์}
\idxen{Molecular Property Prediction!Potential Energy Surface}
%--------------------------

การทำนายพื้นผิวพลังงานศักย์ (Potential Energy Surface หรือ PES)

Machine Learning Potentials (MLP) แปลได้ตรงตัวคือ \textit{การเรียนรู้พลังงานศักย์} ซึ่งเป็นอีกหนึ่งเครื่องมือสำคัญสำหรับการจำลอง%
ในระดับอะตอม (Atomistic Simulation) โดยเฉพาะการศึกษาพลังงานศักย์ของโมเลกุล โดยมีงานวิจัยที่ได้มีการพัฒนาทั้ง Representation 
และอัลกอริทึม ML\autocite{behler2016,botu2017,brockherde2017} ตัวอย่างของงานวิจัยเฉพาะทางที่ใช้ MLP 
เช่น การศึกษาพลังงานศักย์ระหว่างอะตอมเพื่อเพิ่มความแม่นยำในการจำลองพลวัตเชิงโมเลกุล (Molecular Dynamics หรือ MD)%
\autocite{deringer2019,mortazavi2020,zuo2020,dhaliwal2022}, หรือการพัฒนา Force Field สำหรับการจำลองแบบดั้งเดิม 
(Classical MD)\autocite{noe2020} และสำหรับการจำลองแบบเริ่มแรก (\textit{ab initio} MD หรือ AIMD)%
\autocite{sivaraman2020,schran2021,li2022}
\idxboth{พลวัตเชิงโมเลกุล}{Molecular Dynamics}
\idxboth{พลวัตเชิงโมเลกุล!แบบดั้งเดิม}{Molecular Dynamics!Classical Molecular Dynamics}
\idxboth{พลวัตเชิงโมเลกุล!แบบเริ่มแรก}{Molecular Dynamics!\textit{ab initio} Molecular Dynamics}

MLP แบ่งออกได้เป็นสองประเภทตามรูปแบบของอัลกอริทึมของ ML คือแบบเคอร์เนล (Kernel-based Potentials) และแบบโครงข่ายประสาท 
(Neural Network-based Potentials)

%--------------------------
\subsection{การทำนายพลังงานศักย์ด้วยวิธีเชิงเคอร์เนล}
\label{ssec:pred_pot_ener_kernel}
\idxth{การทำนายคุณสมบัติของโมเลกุล!พื้นผิวพลังงานศักย์!วิธีเชิงเคอร์เนล}
\idxen{Molecular Property Prediction!Potential Energy Surface!Kernel-based}
%--------------------------

เรามาดูรายละเอียดของ Kernel-based Potentials กันก่อน ซึ่งผู้เขียนขอยกมาให้ดู 3 วิธี

\paragraph{Gaussian Approximation Potentials (GAP)} เป็นวิธีที่นำเสนอโดย Albert P. Bart\'{o}k และคณะ ซึ่งตีพิมพ์บทความ%
วิจัยเรื่อง \enquote{Gaussian Approximation Potentials: The Accuracy of Quantum Mechanics, without the Electrons} 
ในวารสาร Physical Review Letters\autocite{bartok2010} GAP เป็นวิธีคำนวณที่ใช้สำหรับการทำนายพลังงานและแรงของแต่ละอะตอม%
ภายในโมเลกุลโดยที่ GAP จะทำการเรียนรู้ค่าทั้งสองจาก PES ซึ่งมักจะได้มาจากการคำนวณด้วยวิธีง่าย ๆ ทั่วไป เช่น DFT 
\idxen{Gaussian Approximation Potentials (GAP)}

GAP นั้นเป็นการทำ Kernel Regression ของ PES แบบ Non-parametric ซึ่งเป็นการประมาณค่าของพลังงานแบบเฉพาะที่ (Local Energy)
ดังนี้

\begin{equation}\label{eq:gap}
    \bar{\epsilon}_{\ast} = \delta^{2} \sum\limits_{s = 1}^{N_s} \alpha_{s} k(\ast,s)
\end{equation}

\noindent โดยที่ $\ast$ หมายถึง Environment ของอะตอมที่เราต้องการทำนาย, $\delta$ คือพารามิเตอร์ที่กำหนดสเกลของพลังงาน, 
$s$ คือจำนวนของ Configuration (ขนาดของ Training Set), $\alpha_{s}$ คือสัมประสิทธิ์ของการ Fitting และ $k(\ast,s)$ 
คือเคอร์เนล

โดยทริคของ GAP คือทำการแปลง PES ที่ปกติจะเป็นฟังก์ชันแบบไม่เป็นเชิงเส้นที่ขึ้นกับตำแหน่งของอะตอม $(E = E(\{ \bm{r}_i \}))$ 
ให้กลายเป็นฟังก์ชันแบบเชิงเส้นที่ขึ้นกับเคอร์เนลแทน พอเรามีฟังก์ชันแบบเชิงเส้นแล้วเราก็สามารถทำ Linear Regression เพื่อหาค่า Parameter 
ต่าง ๆ ที่ใช้ในการ Fit ได้นั่นเอง โดยเคอร์เนลในที่นี้คือการวัดความคล้ายคลึงกันระหว่าง Feature ของอะตอมแต่ละตัวในโมเลกุล (Atomic 
Environment)

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig/gap_procedure.jpeg}
    \caption{ขั้นตอนการคำนวณ Representation และการใช้ Gaussian Approximation Potentials ในการทำนายพลังงานและแรง
    (เครดิตภาพ: \textit{Chem. Rev.}, \textbf{2021}, 121, 10073\autocite{deringer2019})}
    \label{fig:gap_procedure}
\end{figure}

ขั้นตอนของการทำ GAP นั้นสามารถดูได้ตามภาพที่ \ref{fig:gap_procedure} โดยมีขั้นตอนดังต่อไปนี้
\begin{enumerate}
    \item เริ่มต้นเราทำการคำนวณหา Representation ก่อนโดยใช้ Atomic Descriptor ซึ่งเป็นข้อมูลหรือ Feature ในระดับอะตอม
    \item หลังจากนั้นทำ Gaussian Process Regression ซึ่งเป็นหัวใจของ GAP
    \item เราจะได้ฟังก์ชันที่สามารถนำไปใช้ในการทำนายพลังงานและแรงเชิงอะตอมได้ ซึ่งสุดท้ายก็สามารถนำมาหาพลังงานรวมของโมเลกุลได้
\end{enumerate}

เนื่องจากว่า GAP ไม่ได้ขึ้นกับฟังก์ชันที่มีรูปแบบตายตัวเหมือนกับ Harmonic Potentials ดังนั้น GAP จึงมีความยืดหยุ่นต่อชุดข้อมูลที่ใช้ในการ%
ฝึกสอน สามารถอธิบายการเปลี่ยนแปลงทางเคมี เช่น การสร้างพันธะหรือการสลายพันธะ ซึ่งเป็นสิ่งที่สนามแรง (Force Field) แบบดั้งเดิมหรือ%
ทั่วไปนั่นทำได้ยาก\autocite{ceriotti2018}

\paragraph{Moment Tensor Potentials (MTP)}
\autocite{shapeev2016}
    
\paragraph{Spectral Neighbor Analysis Potentials (SNAP)} 
\autocite{thompson2015,deng2019,cusentino2020,domina2022}

%--------------------------
\subsection{การทำนายพลังงานศักย์ด้วยวิธีเชิงโครงข่ายประสาท}
\label{ssec:pred_pot_ener_nn}
\idxth{การทำนายคุณสมบัติของโมเลกุล!พื้นผิวพลังงานศักย์!วิธีเชิงโครงข่ายประสาท}
\idxen{Molecular Property Prediction!Potential Energy Surface!Neural Network-based}
%--------------------------

\paragraph{High-dimensional Neural Network Potentials (HDNNP)}\autocite{behler2007} เป็นสถาปัตยกรรม Neural Network 
ที่ได้รับความนิยมมากในการทำนายพลังงานหรือศักย์ของโมเลกุล ซึ่งโมเดล HDNNP นั้นมีชื่อเรียกอีกชื่อว่า Behler-Parrinello Neural Network 
(BPNN) แนวคิดของ BPNN นั้นก็คือการอธิบายพลังงานรวมของโมเลกุลโดยทำให้อยู่ในรูปของผลรวมเชิงเส้นของพลังงานย่อยของแต่ละอะตอมหรือ 
Atomic Contribution โดย Neural Network ที่ถูกสร้างขึ้นมาและใช้ใน BPNN นั้นมีขนาดหรือจำนวนของหน่วยการเรียนรู้ที่สอดคล้องกับจำนวน%
อะตอมภายในโมเลกุล

\paragraph{ANAKIN-ME} เรียกสั้น ๆ ว่า ANI (ชื่อเต็มคือ Accurate NeurAl networK engINe for Molecular Energies) เป็นโมเดล 
Neural Network ที่ถูกพัฒนาขึ้นมาโดยใช้ BPNN โดยโมเดลในตระกูลของ ANI นั้นมีด้วยกันหลายโมเดลซึ่งสามารถทำนายพลังงานของโมเลกุลได้%
เทียบเท่าหรือเทียบเคียงกับการคำนวณด้วยวิธีทางเคมีควอนตัมแบบดั้งเดิม เช่น วิธี Coupled Cluster

\begin{itemize}
    \item ANI-1x\autocite{smith2017}
    
    \item ANI-1ccx\autocite{smith2018}
    
    \item ANI-2x\autocite{smith2019,devereux2020}
\end{itemize}

%--------------------------
\section{การทำนายพลังงานการทำให้เกิดอะตอมและพลังงานของออร์บิทัล}
\label{sec:pred_ener_atom_orb}
\idxth{การทำนายคุณสมบัติของโมเลกุล!พลังงานการทำให้เกิดเป็นอะตอม}
\idxen{Molecular Property Prediction!Atomization Energy}
\idxth{การทำนายคุณสมบัติของโมเลกุล!พลังงาน HOMO}
\idxen{Molecular Property Prediction!HOMO Energy}
\idxth{การทำนายคุณสมบัติของโมเลกุล!พลังงาน LUMO}
\idxen{Molecular Property Prediction!LUMO Energy}
%--------------------------

พลังานการทำให้เกิดอะตอม (Atomization Energy) และพลังงานของออร์บิทัล HOMO และ LUMO เป็นคุณสมบัติของโมเลกุลที่นักเคมีให้ความสนใจ%
เพราะว่าทั้งสองพลังงานนี้เป็นสิ่งสำคัญที่ช่วยให้เราเข้าใจความเสถียรของโมเลกุลและออร์บิทัลที่เกี่ยวข้องกับการสร้างพันธะในปฏิกิริยาเคมี

\begin{table}[H]
    \begin{threeparttable}[b]
    \centering
    \caption{ค่าความคลาดเคลื่อน (Mean Absolute Error หรือ MAE) ของพลังงานการทำให้เกิดอะตอม $(U_{0})$ ในหน่วย kcal/mol 
    และพลังงานของออร์บิทัลชั้นที่สูงสุดที่มีอิเล็กตรอน (HOMO) และชั้นที่ต่ำที่สุดที่ไม่มีอิเล็กตรอน (LUMO) ในหน่วย eV ที่ทำนายด้วยอัลกอริทึม 
    Machine Learning (ML) เช่น Kernel Ridge Regression (KRR), Elastic Net (EN), Gaussian Process Regression 
    (GPR)}
    \label{tab:pred_ener_atom_orb_ml}
    \begin{tabular}{lcccl}
    \toprule
    \textbf{โมเดล ML} &\textbf{$U_{0}$} &HOMO &LUMO &\textbf{อ้างอิง} \\
    \midrule
    KRR/CM\tnote{1} &9.9 &- &- &Rupp และคณะ\autocite{rupp2012} \\
    Multilayer NN/Random CM\tnote{1} &3.5 &- &- &Montavon และคณะ\autocite{montavon2012} \\
    Multitask NN/Random CM\tnote{2} &3.7 &0.15 &0.12 &Montavon และคณะ\autocite{montavon2013} \\
    KRR/Random CM\tnote{1} &3 &- &- &Hansen และคณะ\autocite{hansen2013} \\
    KRR/BoB\tnote{1} &1.5 &- &- &Hansen และคณะ\autocite{hansen2015} \\
    KRR/BoB\tnote{2} &1.8 &0.15 &0.16 &Huang และ von Lilienfeld\autocite{huang2016} \\
    KRR/BAML\tnote{2} &1.2 &0.1 &0.11 &Huang และ von Lilienfeld\autocite{huang2016} \\
    KRR/REMatch-SOAP\tnote{2} &0.92 &0.11 &0.08 &De และคณะ\autocite{de2016} \\
    EN/CM\tnote{3} &21 &0.34 &0.63 &Faber และคณะ\autocite{faber2017} \\
    EN/BoB\tnote{3} &13.9 &0.28 &0.52 &Faber และคณะ\autocite{faber2017} \\
    KRR/CM\tnote{3} &3 &0.13 &0.18 &Faber และคณะ\autocite{faber2017} \\
    KRR/BoB\tnote{3} &1.5 &0.09 &0.12 &Faber และคณะ\autocite{faber2017} \\
    KRR/BAML\tnote{3} &1.2 &0.09 &0.12 &Faber และคณะ\autocite{faber2017} \\
    KRR/HDAD\tnote{3} &0.6 &0.07 &0.08 &Faber และคณะ\autocite{faber2017} \\
    KRR/MBTR\tnote{2} &0.6 &- &- &Huo และ Rupp\autocite{huo2022} \\
    GPR/SOAP-GAP\tnote{2} &0.4 &- &- &Bart\'{o}k และคณะ\autocite{bartok2017} \\
    KRR/SOAP multi-kernel\tnote{3} &0.14 &- &- &Willatt และคณะ\autocite{willatt2018} \\
    KRR/FCHL19\tnote{3} &0.25 & & &Christensen และคณะ\autocite{christensen2020} \\
    AML/amon\tnote{3} &<1.0 & & &Huang และ von Lilienfeld\autocite{huang2020} \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \item [1]ชุดข้อมูล QM7
        \item [2]ชุดข้อมูล QM7b
        \item [3]ชุดข้อมูล QM9
      \end{tablenotes}
    \end{threeparttable}
\end{table}

ตารางที่ \ref{tab:pred_ener_atom_orb_ml} และ \ref{tab:pred_ener_atom_orb_nn} แสดงการเปรียบเทียบค่าความถูกต้องของโมเดล
ML และ NN ในการทำนายพลังงาน โดยค่า Mean Absolute Error (MAE) ซึ่งเป็นค่าที่บอกความคลาดเคลื่อนนั้นแสดงให้เห็นว่าอัลกอริทึม KRR 
สามารถทำนายค่าพลังงานทำให้เกิดอะตอมและพลังงาน HOMO และ LUMO ได้แม่นยำมากโดยมีค่าที่ใกล้เคียงกับความถูกต้องทางเคมี (1 kcal/mol) 
โดยใช้ BoB\autocite{huang2016} และ BAML\autocite{faber2017} เป็น Representation ในการฝึกสอน

\begin{table}[H]
    \begin{threeparttable}[b]
    \centering
    \caption{ค่าความคลาดเคลื่อน (Mean Absolute Error หรือ MAE) ของพลังงานการทำให้เกิดอะตอม $(U_{0})$ ในหน่วย kcal/mol 
    และพลังงานของออร์บิทัลชั้นที่สูงสุดที่มีอิเล็กตรอน (HOMO) และชั้นที่ต่ำที่สุดที่ไม่มีอิเล็กตรอน (LUMO) ในหน่วย eV ที่ทำนายด้วยอัลกอริทึม 
    Neural Network (NN) เช่น Message-passing Neural Network (MPNN)}
    \label{tab:pred_ener_atom_orb_nn}
    \begin{tabular}{lcccl}
    \toprule
    \textbf{โมเดล NN} &\textbf{$U_{0}$} &HOMO &LUMO &\textbf{อ้างอิง} \\
    \midrule
    MPNN\tnote{1} &0.45 &0.99 &0.87 &Gilmer และคณะ\autocite{gilmer2017} \\
    ANI-1a NN/ACSF\tnote{2} &<1.5 &- &- &Smith และคณะ\autocite{smith2017,smith2018,smith2019} \\
    HDNN/w-ACSF\tnote{1} &<1.8 &- &- &Gastegger และคณะ\autocite{gastegger2018} \\
    Multitask NN/CM\tnote{1} &44 &0.38 &0.63 &Hou และคณะ\autocite{hou2018} \\
    SchNet NN\tnote{1} &0.32 &0.04 &0.03 &Schütt และคณะ\autocite{schutt2018} \\
    HIP-NN\tnote{1} &0.26 &- &- &Lubbers และคณะ\autocite{lubbers2018} \\
    HDNN\tnote{1} &0.41 &- &- &Unke และ Meuwly\autocite{unke2018} \\
    RNN/VAE\tnote{1} &- &0.16 &0.16 &G\'{o}mez-Bombarelli และคณะ\autocite{gomez-bombarelli2018} \\
    PhysNet NN\tnote{1} &0.14 &- &- &Unke และ Meuwly\autocite{unke2019} \\
    SchNOrb NN\tnote{3} &<0.046 &<0.02 &<0.1 &Sch\"{u}tt และคณะ\autocite{schutt2019a} \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \item [1]ชุดข้อมูล QM9
        \item [2]ชุดข้อมูล COMP6
        \item [3]ชุดข้อมูล MD17
      \end{tablenotes}
    \end{threeparttable}
\end{table}

\autocite{pereira2017,stuke2019,chang2019,rahaman2020,moore2022,ye2022}

%--------------------------
\section{การจำลองสนามแรง}
\label{sec:model_ff}
\idxth{การทำนายคุณสมบัติของโมเลกุล!สนามแรง}
\idxen{Molecular Property Prediction!Force Field}
%--------------------------

การทำนายหรือพยากรณ์แรง (Forces) และพลังงาน (Energies) ของโมเลกุลนั้นเรียกอีกอย่างหนึ่งว่าการสร้างโมเดล Machine Learning 
Force Field (MLFF) เริ่มต้นสมมติว่าเรามีชุดข้อมูลที่มี Feature Vector ซึ่งเขียนแทนด้วย $\bm{D}$ และมีอนุพันธ์แบบเวกเตอร์ 
(Divergence) เป็น $\nabla_{\bm{r}_{i}} \bm{D}$ และมีข้อมูลเพิ่มเติมคือพลังงาน $E$ และแรง $\bm{F}$ ของระบบ 
(โมเลกุล) สำหรับการฝึกสอนเราจะสร้าง Neural Network $(f)$ เพื่อสร้างโมเดลสำหรับการทำนายพลังงาน $(\hat{E} = f(\bm{D}))$%
\footnote{เครื่องหมาย\, $\hat{}$\, (อ่านว่า \enquote{hat}) ที่อยู่ด้านบนของตัวแปรเป็นส่งที่บ่งบอกว่าค่าตัวแปรนั้นสิ่งที่เราทำนายออกมา} 
ซึ่งเราสามารถคำนวณแรงได้โดยตรงจากค่าติดลบของ Gradient ของพลังงานเทียบกับพิกัดตำแหน่งของอะตอมนั้น ๆ ดังนั้นแรงที่ได้จะเป็นปริมาณ%
ต่ออะตอม ยกตัวอย่างเช่นแรงของอะตอม $i$ สามารถคำนวณได้จากสมการต่อไปนี้ (โดยใช้เวกเตอร์แบบแถว)
\idxen{Force Field}

\begin{align}\label{eq:force_pred}
    \hat{\bm{F}}_i &= - \nabla_{\bm{r}_{i}} f(\bm{D}) \\
    &= - \nabla_{\bm{D}} f \cdot \nabla_{\bm{r}_{i}} \bm{D}\\
    &= - \begin{bmatrix}
        \frac{\partial f}{\partial D_{1}} & \frac{\partial f}{\partial D_{2}} & \dots
    \end{bmatrix}
    \begin{bmatrix}
        \frac{\partial D_{1}}{\partial x_{i}} & \frac{\partial D_{1}}{\partial y_i} & 
            \frac{\partial D_{1}}{\partial z_i}\\
        \frac{\partial D_{2}}{\partial x_{i}} & \frac{\partial D_{2}}{\partial y_i} & 
            \frac{\partial D_{2}}{\partial z_i}\\
        \vdots & \vdots & \vdots \\
    \end{bmatrix}
\end{align}

\noindent จากสมการ \ref{eq:force_pred} นั้นเราอธิบายได้ว่า $\nabla_{\bm{D}} f$ เป็นค่าอนุพันธ์ของคำตอบของโมเดล ML 
ซึ่งจะเทียบกับ Descriptor $\bm{D}$ และ $\nabla_{\bm{r}_{i}} \bm{D}$ เป็นอนุพันธ์ของ Descriptor ที่เทียบกับ%
ตำแหน่งของอะตอมซึ่งตามที่เราได้ศึกษามาก่อนหน้านี้ว่า Neural Network นั้นจะให้คำตอบที่เป็นแบบ Analytical Solution

โค้ดดังต่อไปนี้คือการใช้ไลบรารี่ DScribe ในการสร้างชุดข้อมูลโดยมี SOAP เป็น Descriptor สำหรับคำนวณ Feature Vector แล้วฝึกสอน%
โมเดล Fully Connected Neural Network เพื่อเรียนรู้ MLFF ต่อไป

\textbf{ขั้นตอนที่ 1} สร้างชุดข้อมูลของพลังงานและแรงของ Lennard-Jones

\begin{lstlisting}[style=MyPython]
import numpy as np
import ase
from ase.calculators.lj import LennardJones
import matplotlib.pyplot as plt
from dscribe.descriptors import SOAP

# Setting up the SOAP descriptor
soap = SOAP(
    species=["H"],
    periodic=False,
    rcut=5.0,
    sigma=0.5,
    nmax=3,
    lmax=0,
)

n_samples = 200
traj = []
n_atoms = 2
energies = np.zeros(n_samples)
forces = np.zeros((n_samples, n_atoms, 3))
r = np.linspace(2.5, 5.0, n_samples)

for i, d in enumerate(r):
    a = ase.Atoms('HH', positions = [[-0.5 * d, 0, 0], [0.5 * d, 0, 0]])
    a.set_calculator(LennardJones(epsilon=1.0 , sigma=2.9))
    traj.append(a)
    energies[i] = a.get_total_energy()
    forces[i, :, :] = a.get_forces()
	
# Plot the energies to validate them
fig, ax = plt.subplots(figsize=(8, 5))
plt.subplots_adjust(left=0.1, right=0.95, top=0.95, bottom=0.1)
line, = ax.plot(r, energies)
plt.xlabel("Distance (Å)")
plt.ylabel("Energy (eV)")
plt.show()

# Create the SOAP desciptors and their derivatives for all samples. 
# One center is chosen to be directly between the atoms.
derivatives, descriptors = soap.derivatives(
    traj,
    positions=[[[0, 0, 0]]] * len(r),
    method="analytical"
)

# Save to disk for later training
np.save("r.npy", r)
np.save("E.npy", energies)
np.save("D.npy", descriptors)
np.save("dD_dr.npy", derivatives)
np.save("F.npy", forces)
\end{lstlisting}

\vspace{1em}

\textbf{ขั้นตอนที่ 2} นำเข้าชุดข้อมูลและเตรียมอินพุตสำหรับฝึกสอนโมเดล

\begin{lstlisting}[style=MyPython]
import numpy as np
import torch
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
torch.manual_seed(7)

# Load the dataset (We only have one SOAP center)
D_numpy = np.load("D.npy")[:, 0, :]
n_samples, n_features = D_numpy.shape
E_numpy = np.array([np.load("E.npy")]).T
F_numpy = np.load("F.npy")
dD_dr_numpy = np.load("dD_dr.npy")[:, 0, :, :, : ]  
r_numpy = np.load("r.npy")

# Select equally spaced points for training
n_train = 30
idx = np.linspace(0, len(r_numpy) - 1, n_train).astype(int)
D_train_full = D_numpy[idx]
E_train_full = E_numpy[idx]
F_train_full = F_numpy[idx]
r_train_full = r_numpy[idx]
dD_dr_train_full = dD_dr_numpy[idx]

# Standardize input for improved learning. 
# Fit is done only on training data, 
# scaling is applied to both descriptors and 
# their derivatives on training and test sets.

scaler = StandardScaler().fit(D_train_full)
D_train_full = scaler.transform(D_train_full)
D_whole = scaler.transform(D_numpy)
dD_dr_whole = dD_dr_numpy / scaler.scale_[None, None, None, :]
dD_dr_train_full = dD_dr_train_full / scaler.scale_[None, None, None, :]

# Calculate the variance of energy and force values for 
# the training set. These are used to balance their 
# contribution to the MSE loss
var_energy_train = E_train_full.var()
var_force_train = F_train_full.var()

# Subselect 20% of validation points for early stopping.
D_train, D_valid, E_train, E_valid, F_train, F_valid, dD_dr_train, dD_dr_valid = train_test_split(
    D_train_full,
    E_train_full,
    F_train_full,
    dD_dr_train_full,
    test_size=0.2,
    random_state=7,
)

# Create tensors for pytorch
D_whole = torch.Tensor(D_whole)
D_train = torch.Tensor(D_train)
D_valid = torch.Tensor(D_valid)
E_train = torch.Tensor(E_train)
E_valid = torch.Tensor(E_valid)
F_train = torch.Tensor(F_train)
F_valid = torch.Tensor(F_valid)
dD_dr_train = torch.Tensor(dD_dr_train)
dD_dr_valid = torch.Tensor(dD_dr_valid)
\end{lstlisting}

\vspace{1em}

\textbf{ขั้นตอนที่ 3} สร้างโมเดลและกำหนด Loss Function 

\begin{lstlisting}[style=MyPython]
class FFNet(torch.nn.Module):
    """A simple feed-forward network with one hidden layer, randomly
    initialized weights, sigmoid activation and a linear output layer.
    """
    def __init__(self, n_features, n_hidden, n_out):
        super(FFNet, self).__init__()
        self.linear1 = torch.nn.Linear(n_features, n_hidden)
        torch.nn.init.normal_(self.linear1.weight, mean=0, std=1.0)
        self.sigmoid = torch.nn.Sigmoid()
        self.linear2 = torch.nn.Linear(n_hidden, n_out)
        torch.nn.init.normal_(self.linear2.weight, mean=0, std=1.0)

    def forward(self, x):
        x = self.linear1(x)
        x = self.sigmoid(x)
        x = self.linear2(x)

        return x


def energy_force_loss(E_pred, E_train, F_pred, F_train):
    """Custom loss function that targets both energies and forces.
    """
    energy_loss = torch.mean((E_pred - E_train)**2) / var_energy_train
    force_loss = torch.mean((F_pred - F_train)**2) / var_force_train
    return energy_loss + force_loss

# Initialize model
model = FFNet(n_features, n_hidden=5, n_out=1)

# The Adam optimizer is used for training the model parameters
optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
\end{lstlisting}

\vspace{1em}

\textbf{ขั้นตอนที่ 4} ฝึกสอนโมเดล Neural Network โดยใช้ PyTorch

\begin{lstlisting}[style=MyPython]
n_max_epochs = 5000
batch_size = 2
patience = 20
i_worse = 0
old_valid_loss = float("Inf")
best_valid_loss = float("Inf")

# We explicitly require that the gradients should be 
# calculated for the input variables. PyTorch will 
# not do this by default as it is typically not needed.
D_valid.requires_grad = True

# Epochs
for i_epoch in range(n_max_epochs):
    # Batches
    permutation = torch.randperm(D_train.size()[0])
    for i in range(0, D_train.size()[0], batch_size):
        indices = permutation[i:i+batch_size]
        D_train_batch, E_train_batch = D_train[indices], E_train[indices]
        D_train_batch.requires_grad = True
        F_train_batch, dD_dr_train_batch = F_train[indices], dD_dr_train[indices]

        # Predict energies from the descriptor input
        E_train_pred_batch = model(D_train_batch)

        # Get derivatives of model output w.r.t. input variables.
        df_dD_train_batch = torch.autograd.grad(
            outputs=E_train_pred_batch,
            inputs=D_train_batch,
            grad_outputs=torch.ones_like(E_train_pred_batch),
            create_graph=True,
        )[0]

        # Get derivatives of input variables (=descriptor) 
        # w.r.t. atom positions = forces
        F_train_pred_batch = -torch.einsum('ijkl,il->ijk', dD_dr_train_batch, df_dD_train_batch)

        # Zero gradients, perform a backward pass, 
        # and update the weights.
        # D_train_batch.grad.data.zero_()
        optimizer.zero_grad()
        loss = energy_force_loss(E_train_pred_batch, E_train_batch, F_train_pred_batch, F_train_batch)
        loss.backward()
        optimizer.step()
            
    # Check early stopping criterion and save best model
    E_valid_pred = model(D_valid)
    df_dD_valid = torch.autograd.grad(
        outputs=E_valid_pred,
        inputs=D_valid,
        grad_outputs=torch.ones_like(E_valid_pred),
    )[0]
    F_valid_pred = -torch.einsum('ijkl,il->ijk', dD_dr_valid, df_dD_valid)
    valid_loss = energy_force_loss(E_valid_pred, E_valid, F_valid_pred, F_valid)
    if valid_loss < best_valid_loss:
        # print("Saving at epoch {}".format(i_epoch))
        torch.save(model.state_dict(), "best_model.pt")
        best_valid_loss = valid_loss

    if valid_loss >= old_valid_loss:
        i_worse += 1
    else:
        i_worse = 0

    if i_worse > patience:
        print("Early stopping at epoch {}".format(i_epoch))
        break
    old_valid_loss = valid_loss

    if i_epoch % 500 == 0:
        print("  Finished epoch: {} with loss: {}".format(i_epoch, loss.item()))
\end{lstlisting}

\vspace{1em}

\textbf{ขั้นตอนที่ 5} ประเมินโมเดลและวิเคราะห์พลังงานและแรงที่ทำนายได้

\begin{lstlisting}[style=MyPython]
model.load_state_dict(torch.load("best_model.pt"))
model.eval()

# Calculate energies and force for the entire range
E_whole = torch.Tensor(E_numpy)
F_whole = torch.Tensor(F_numpy)
dD_dr_whole = torch.Tensor(dD_dr_whole)
r_whole = r_numpy
D_whole.requires_grad = True
E_whole_pred = model(D_whole)
df_dD_whole = torch.autograd.grad(
    outputs=E_whole_pred,
    inputs=D_whole,
    grad_outputs=torch.ones_like(E_whole_pred),
)[0]

F_whole_pred = -torch.einsum('ijkl,il->ijk', dD_dr_whole, df_dD_whole)
\end{lstlisting}

\vspace{1em}

\textbf{ขั้นตอนที่ 6} พลอตค่าพลังงาน, แรง, และกราฟการฝึกสอนโมเดล

\begin{lstlisting}[style=MyPython]
# Plot energies for the whole range
E_whole_pred = E_whole_pred.detach().numpy()
E_whole = E_whole.detach().numpy()
order = np.argsort(r_whole)
fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 10))
ax1.plot(r_whole[order], E_whole[order], label="True", linewidth=3, linestyle="-")
ax1.plot(r_whole[order], E_whole_pred[order], label="Predicted", linewidth=3, linestyle="-")
ax1.set_ylabel('Energy', size=15)
mae_energy = mean_absolute_error(E_whole_pred, E_whole)
ax1.text(0.95, 0.5, "MAE: {:.2} eV".format(mae_energy), size=16, horizontalalignment='right', verticalalignment='center', transform=ax1.transAxes)

# Plot forces for whole range
F_x_whole_pred = F_whole_pred.detach().numpy()[order, 0, 0]
F_x_whole = F_whole[:, 0, 0][order]
ax2.plot(r_whole[order], F_x_whole, label="True", linewidth=3, linestyle="-")
ax2.plot(r_whole[order], F_x_whole_pred, label="Predicted", linewidth=3, linestyle="-")
ax2.set_xlabel('Distance', size=15)
ax2.set_ylabel('Forces', size=15)
mae_force = mean_absolute_error(F_x_whole_pred, F_x_whole)
ax2.text(0.95, 0.5, "MAE: {:.2} eV/Å".format(mae_force), size=16, horizontalalignment='right', verticalalignment='center', transform=ax2.transAxes)

# Plot training points
F_x_train_full = F_train_full[:, 0, 0]
ax1.scatter(r_train_full, E_train_full, marker="o", color="k", s=20, label="Training points", zorder=3)
ax2.scatter(r_train_full, F_x_train_full, marker="o", color="k", s=20, label="Training points", zorder=3)

# Show plot
ax1.legend(fontsize=12)
plt.subplots_adjust(left=0.08, right=0.97, top=0.97, bottom=0.08, hspace=0)
plt.show()
\end{lstlisting}

\vspace{1em}

จากตัวอย่างโค้ดด้านบนทั้ง 6 ขั้นตอนนั้นเป็นวิธีการใช้ ML ในการสร้างชุดข้อมูลและฝึกสอนโมเดลเพื่อสร้างโมเดล Force Field ที่สามารถนำไปใช้%
งานต่อได้จริงในการจำลองเชิงโมเลกุลต่อไป เช่น นำไปใช้กับ Molecular Dynamics เพื่อเพิ่มความเร็วในการคำนวณ

นอกจากนี้ยังมีโมเดล ML ที่น่าสนใจที่ถูกพัฒนาขึ้นมาเพื่อเรียนรู้ Force Field โดยเฉพาะ ดังนี้

\begin{enumerate}[topsep=0pt]
    \item FFLUX\autocite{hughes2019}
    
    \item TensorMol\autocite{yao2018}
\end{enumerate}

%--------------------------
\section{การทำนายพลังงานสหสัมพันธ์ของอิเล็กตรอน}
\label{sec:pred_corre_ener}
\idxth{การทำนายคุณสมบัติของโมเลกุล!พลังงานสหสัมพันธ์ของอิเล็กตรอน}
\idxen{Molecular Property Prediction!Correlation Energy}
%--------------------------

\begin{framed}
    ผู้อ่านอยากมี Exchange-Correlation Functional ที่ใช้อธิบายพลังงานอันตรกิริยาระหว่างอิเล็กตรอนสำหรับ DFT เป็นของตัวเองไหมครับ? 
\end{framed}

เหตุผลที่ผู้เขียนเริ่มต้นหัวข้อนี้ด้วยคำถามนี้ก็เพราะว่าเราสามารถใช้ ML สร้าง Exchange-Correlation หรือ XC Functional ของเราเองได้นั่นเอง 
โดยไม่ได้เป็นการหลอกลวงผู้อ่านแต่อย่างใดเพราะสามารถทำได้จริง อย่างที่ทราบกันดีว่าในการคำนวณ DFT นั้นเราจะต้องทำการเลือก XC Functional 
ที่ต้องการใช้ซึ่งปกติแล้วเราก็ลองผิดลองถูกกันไป\footnote{จริง ๆ จะบอกว่าลองแบบมั่ว ๆ ก็ไม่ได้เพราะว่าในปัจจุบันมีบทความรีวิวที่อธิบายจุดเด่น%
และจุดด้อยของแต่ละ XC Functional รวมไปถึง Protocol สำหรับการเลือกใช้ XC Functional ที่เหมาะสมกับระบบโมเลกุลที่เราต้องการจะศึกษา} 
ถ้า XC Functional อันไหนให้ผลการคำนวณที่แม่นยำและใกล้เคียงกับผลการทดลองมากที่สุดก็ใช้อันนั้นไป เพื่อแก้ปัญหาของการที่ XC Functional 
แบบดั้งเดิมนั้นไม่มีความสามารถในการส่งต่อความสามารถในการเรียนรู้หรือการนำไปใช้ได้กับระบบที่หลากหลาย ในปัจจุบันได้มีงานวิจัยที่พัฒนาอัลกอริทึม 
ML สำหรับการเรียนรู้ XC Functional ที่สามารถนำไปใช้งานกับระบบต่าง ๆ ทางเคมีได้อย่างครอบคลุม เช่น 

\begin{itemize}
    \item ฝึกสอนโมเดล ML โดยใช้ Topological Atom\autocite{mcdonagh2018}
    
    \item ฝึกสอนโมเดล ML โดยใช้ Correlation Energy Density ด้วยวิธี CCSD(T)\autocite{nudejima2019}
    
    \item ฝึกสอนโมเดล ML โดยใช้ Electron Density สำหรับวิธี DFT\autocite{dick2020} และ MP2\autocite{han2021} 
    
    \item สร้างโมเดล Neural Network แบบซับซ้อนเพื่อทำนาย Exchange-Correlation Holes\autocite{cuierrier2021,cuierrier2022}
    
    \item สร้างโมเดล Neural Network (DeepMind 21 หรือ DM21) ที่สามารถเรียนรู้และแก้ปัญหา Charge Delocalization Error 
    ของ DFT ได้\autocite{kirkpatrick2021}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/neuralxc.png}
    \caption{สถาปัตยกรรมของอัลกอริทึม NeuralXC โดยเริ่มต้นจากการใช้ DFT คำนวณ Electron Density แบบ Real Space 
    แล้วทำการคำนวณ Feature Vectors จาก Density แล้วก็ใช้เป็นอินพุตสำหรับ BPN โดยเราจะใช้ Network แบบเดียวกันกับอะตอมทุกตัว%
    เพื่อไม่ให้ Descriptor นั้นขึ้นอยู่กับลำดับของอะตอม (Permutation Invariance) ในระหว่างการฝึกสอนโมเดลนั้นเมื่อพลังงานถูกคำนวณ%
    แล้วก็จะมีการคำนวณอนุพันธ์ของพลังงานโดยเทียบกับความหนาแน่นอิเล็กตรอนโดยใช้ Backpropagation เพื่อหาเทอม XC Potential 
    ต่อไป (เครดิตภาพ: \textit{Nat Commun} 2020, 11, 3509)}
    \label{fig:neuralxc}
\end{figure}

หนึ่งในงานวิจัยที่ผู้เขียนคิดว่าผู้อ่านที่สนใจงานทางด้านการพัฒนา XC Functional สำหรับวิธี DFT ควรอ่านนั้นคืองานที่เสนออัลกอริทึม Supervised 
ML ที่ชื่อว่า NeuralXC\autocite{dick2020} โดยเป็น ML ที่เรียนรู้ทั้งเทอมที่ใช้อธิบายพลังงาน Exchange และพลังงาน Correlation สำหรับ 
DFT (ปกติแล้วเทอม Correlation นั้นจะมีความซับซ้อนมากกว่าเทอม Exchange) ซึ่งในงานวิจัยนี้ก็ได้มีการฝึกสอนโมเดลโดยใช้ความหนาแน่น%
อิเล็กตรอน (Electron Density) ซึ่งงานวิจัยอื่น ๆ ก็มักจะใช้ Electron Density มาเป็นอินพุตเริ่มต้น ซึ่ง Representation ที่ใช้ในงานนี้%
สำหรับการระบุ Electron Density ก็คือ Atom-centered Basis Functions (ACSF) ส่วนอัลกอริทึม ML ที่ใช้คือ Behler-Parrinello 
Neural Networks (BPNN) โดยสมการของ XC Functional ที่โมเดล NeuralXC สร้างออกมาให้เราก็จะอยู่ในรูปของฟังก์ชันกระตุ้น 
(Activation Function) ที่ขึ้น Electron Density $(\rho ({\bm{r}}))$ ตามสมการดังต่อไปนี้ (ตามสมการที่ 5 ในบทความเต็ม) 

\begin{align}
    E_{\text{ML}} [\rho ({\bm{r}})] &= E_{\text{ML}}({\bm{d}}[\rho ({\bm{r}})]) \nonumber \\
    &= \sum_{I}{\epsilon}_{\alpha_{I}}(\bm{d}[\rho (\bm{r}), {\bm{R}}_{I}, \alpha_{I}])
\end{align}

\noindent โดยที่ $\bm{R}_{I}$ คือตำแหน่งของอะตอม $I$ แล้วก็ใช้กฎลูกโซ่ในการหาอนุพันธ์ของพลังงานเทียบกับ Electron Density 
ได้ดังนี้

\begin{align}
    {V}_{{{M}}L}[\rho ({\bf{r}})] &= \frac{\delta {E}_{{{M}}L}[\rho ]}{\delta \rho ({\bf{r}})} \nonumber \\
    &= \sum_{\beta} \frac{\partial {E}_{{{M}}L}}{\partial {c}_{\beta }} 
    \frac{\delta {c}_{\beta }[\rho ]}{\delta \rho ({\bf{r}})}
\end{align}

\noindent โดยเรากระจายอนุพันธ์ของพลังงานให้อยู่ในรูปของอนุพันธ์ของพลังงานเทียบกับ Descriptor ที่เป็นเกี่ยวข้องกับ Basis Function 
$(c_{nlm})$ 

ผู้วิจัยได้ทดสอบ XC Functional ที่ได้จาก NeuralXC กับชุดข้อมูลทางเคมีและอัลกอริทึม ML อื่น ๆ ด้วย เช่น sGDML และมีการเปรียบเทียบ%
ประสิทธิภาพการทำนายกับ Functional อย่าง SCAN และ $\omega$B97M-V ซึ่งเหมาะสำหรับการศึกษาโมเลกุลและโครงสร้างวัสดุโดยเฉพาะ 
นอกจากนี้ XC Functional ที่ได้นั้นก็สามารถทำนาย Electron Density ของโมเลกุลได้อย่างแม่นยำโดยมีค่าความคลาดเคลื่อนเทียบกับวิธี%
การคำนวณขั้นสูงแบบคลาสสิค เช่น CCSD(T) ที่น้อยมาก (เมื่อเทียบกับผลการคำนวณด้วยวิธี DFT ที่ใช้ฟังก์ชันนอล PBE เป็นค่าอ้างอิงหลัก) 
แต่จุดอ่อนอย่างหนึ่งของการใช้ Representation หรือ Loss Function ที่มีความจำเพาะเจาะจงกับค่าเอาต์พุตหรือเป้าหมายมากเกินไปนั้นก็อาจ%
จะส่งผลให้ Transferability ลดลงได้เมื่อนำโมเดลไปใช้ในการทำนายกับโมเลกุลชนิดอื่นที่มีความแตกต่างไปจากโมเลกุลที่นำมาใช้ในการสร้าง%
ชุดข้อมูลที่ใช้ฝึกสอนโมเดล สำหรับผู้อ่านที่สนใจฝึกสอนโมเดลเพื่อคำนวณหา XC Functional เพื่อนำมาใช้ในงานวิจัยสามารถศึกษารายละเอียด%
ในบทความฉบับเต็ม \enquote{Machine Learning Accurate Exchange and Correlation Functionals of the Electronic Density}

นอกการพัฒนาโมเดลแล้วในงานวิจัยนี้ยังได้พัฒนาไลบรารี่สำหรับการสร้าง XC Functional อีกด้วยซึ่งผู้เขียนสนับสนุนงานวิจัยที่เผยแพร่โค้ดเนื่อง%
จากว่างานวิจัยส่วนใหญ่นั้นมักจะโชว์แต่ทฤษฎีแล้วก็ผลการทดลอง แต่มักจะไม่มีโค้ดให้ได้ลองทดสอบ ทำให้เราก็ไม่รู้ว่าวิธีหรือทฤษฎีที่ผู้วิจัยได้เสนอ%
มานั้นถูกต้องไหมหรือมีประสิทธิภาพมากน้อยเพียงใดและยังทำให้นักวิจัยคนอื่น ๆ เข้าถึงงานเราได้มากยิ่งขึ้น

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/ml-xc-conv-network.jpeg}
    \caption{สถาปัตยกรรมของ Three-dimensional Convolutional Neural Network สำหรับการเรียนรู้ XC Potential โดยที่ 
    $\rho$ คือความหนาแน่นอิเล็กตรอนและ $v$ คือ XC Potential (เครดิตภาพ: \textit{J. Phys. Chem. Lett.} 2019, 10, 22, 
    7264-7269)}
    \label{fig:xc_3dcnn}
\end{figure}

งานวิจัยชิ้นที่สองที่น่าสนใจก็คือ \enquote{Toward the Exact Exchange-Correlation Potential: A Three-Dimensional 
Convolutional Neural Network Construct} โดย Zhou และคณะ\autocite{zhou2019} โดยเป็นการใช้ Neural Network ที่เป็น%
แบบคอนโวลูชันแบบสามมิติ (Three-dimensional Convolutional Neural Network หรือ 3D-CNN) ในการเรียนรู้ความสัมพันธ์ระหว่าง%
ความแน่นอิเล็กตรอนแบบที่เกือบจะเป็นเชิงพื้นที่ (Quasi-local Electron Density) และพลังงานศักย์แลกเปลี่ยนและสหสัมพันธ์ (XC Potential) 
โดย Electron Density นั้นได้มาจากการคำนวณด้วยวิธี CCSD ส่วน XC Potential นั้นได้มาจากการคำนวณโดยใช้วิธี Direct Optimization 
ของ Wu และ Yang\autocite{wu2003} ซึ่งเป็นวิธีสำหรับการปรับพลังงานศักย์ (Optimized Effective Potential หรือ OEP)

%--------------------------
\section{การทำนายพลังงานกระตุ้นของปฏิกิริยาเคมี}
\label{sec:pred_act_ener}
\idxth{การทำนายคุณสมบัติของโมเลกุล!พลังงานกระตุ้นของปฏิกิริยาเคมี}
\idxen{Molecular Property Prediction!Activation Energy}
%--------------------------

พลังงานกระตุ้น (Activation Energy) เป็นค่าพลังงานที่ต่ำที่สุดที่ใช้เพื่อกระตุ้นปฏิกิริยาเคมี บ่งบอกถึงความยากง่ายของการเกิดปฏิริยาเคมีที่%
สามารถดำเนินไปได้ ณ สภาวะหนึ่ง ๆ ปัจจัยที่มีผลต่อพลังงานกระตุ้น เช่น ความร้อนและตัวเร่งปฏิกิริยา โดยในปัจจุบันนั้นก็ได้มีการนำ ML เข้ามา%
ช่วยในการทำนายพลังงานกระตุ้นอย่างแพร่หลาย\autocite{lewis-atwell2022} โดยหนึ่งในงานวิจัยนั้นก็คือการใช้ Graph Neural Network 
มาสร้างและฝึกสอนโมเดล\autocite{grambow2020}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/ml_act_energy_regio.jpg}
    \caption{แสดงค่าความแตกต่างระหว่างค่าพลังงานอิสระ $(\Delta\Delta G^{\ddagger})$ ซึ่งถูกเรียนรู้และทำนายด้วยโมเดล ML}
    \label{fig:ml_act_energy_regio}
\end{figure}

นอกจากนี้ยังมีงานวิจัยของ Xin Li และทีมวิจัยที่ได้ใช้โมเดล ML หลายตัวด้วยกันไม่ว่าจะเป็น Linear Regression, Support Vector Regression, 
Random Forest Rregression, Kernel Ridge Regression, Gaussian Process Regression, หรือ Gradient boosting มาใช้ใน%
การทำนายการเลือกเกิดเฉพาะที่ของปฏิริยาเคมี (Regioselectivity) ของปฏิริยาที่เป็นการ Functionalization ของ \ce{C-H} Radical 
ของสารประกอบเฮทเทอโรไซคลิกจำนวน 3,406 ปฏิริยา\autocite{li2020} โดยค่าพลังงานกระตุ้นนั้นถูกเรียนรู้ด้วยโมเดล ML จากค่าความ%
แตกต่างระหว่างพลังงานอิสระกิบส์ (Gibbs Free Energies) ของสารตั้งต้นและสภาวะทรานซิชั่น (Transition State) ซึ่งถูกปรับโครงสร้าง%
ด้วยวิธี DFT โดยใช้ฟังก์ชันนอล B3LYP และ 6-311+G(2d,p) และคำนวณค่าพลังงานด้วย M06-2X และ aug-cc-pVTZ โดย Regioselectivity
ของปฏิริยาเคมีที่ศึกษานั้นถูกคำนวณโดยใช้ความแตกต่างระหว่างค่าพลังงานอิสระ $(\Delta\Delta G^{\ddagger})$

%--------------------------
\section{การทำนายประจุของอะตอม}
\label{sec:pred_atomic_charge}
\idxth{การทำนายคุณสมบัติของโมเลกุล!ประจุของอะตอม}
\idxen{Molecular Property Prediction!Atomic Charge}
%--------------------------

อีกหนึ่งหัวข้องานวิจัยทางเคมีควอนตัมที่ได้รับความนิยมในช่วง 3-4 ปี (ตั้งแต่ปี ค.ศ. 2018) คือการใช้ ML สำหรับการระบุประจุย่อยของอะตอมใน%
โมเลกุล (Partial Atomic Charge Assignment) งานวิจัยโดดเด่นและได้รับการตีพิมพ์ในวารสารชั้นแนวหน้าที่ใช้ ML เข้ามาในการทำนายหรือ%
ระบุประจุของอะตอมที่ผู้เขียนได้เลือกมาซึ่งเรียงลำดับตาม Timeline มีดังนี้

\begin{enumerate}
    \item \enquote{Fast and Accurate Generation of \textit{ab initio} Quality Atomic Charges Using Nonparametric 
    Statistical Regression}\autocite{rai2013} \\ 
    หนึ่งในงานวิจัยแรก ๆ ที่เริ่มมีการประยุกต์เทคนิคทางสถิติ (Regression) เข้ามาใช้ในการสร้างโมเดลเรียนรู้การทำนายประจุย่อย
    
    \item \enquote{Machine Learning of Partial Charges Derived from High-Quality Quantum-Mechanical Calculations} 
    \autocite{bleiziffer2018} \\
    งานวิจัยที่ต่อยอดมาจากงานของ โดยใช้ Feature ที่ชื่อว่า \enquote{Atom-centered Atom-pairs Fingerprint}\autocite{carhart1985} 
    ในการฝึกสอนโมเดลและนำมาใช้ในทำนายประจุย่อยได้อย่างแม่นยำมาก ๆ
    
    \item \enquote{Transferable Dynamic Molecular Charge Assignment Using Deep Neural Networks}\autocite{nebgen2018} \\
    งานวิจัยนี้ใช้ Neural Network ในการทำนายประจุย่อยแบบไดนามิกส์
    
    \item \enquote{PhysNet: A Neural Network for Predicting Energies, Forces, Dipole Moments, and Partial Charges}
    \autocite{unke2019} \\
    งานวิจัยนี้นำเสนอโมเดล ML ที่ชื่อว่า PhysNet ซึ่งสามารถทำนายคุณสมบัติเชิงโมเลกุล ประกอบไปด้วย พลังงาน, แรง, ไดโพลโมเมนต์, 
    และประจุย่อย
    
    \item \enquote{Fast and Accurate Machine Learning Strategy for Calculating Partial Atomic Charges in 
    Metal-Organic Frameworks}\autocite{kancharlapalli2021} \\ 
    งานวิจัยนี้เป็นการทำนายประจุของอะตอมในสารประกอบ Metal-Organic Frameworks (MOFs)
    
    \item \enquote{High-Precision Atomic Charge Prediction for Protein Systems Using Fragment Molecular Orbital 
    Calculation and Machine Learning}\autocite{kato2020} \\ 
    งานวิจัยนี้สนใจการทำนายประจุเชิงอะตอมของโปรตีโดยใช้ Fragment Molecular Orbital
    
    \item \enquote{DeepChargePredictor: A Web Server for Predicting QM-based Atomic Charges via State-of-the-art 
    Machine-learning Algorithms}\autocite{wang2021} \\ 
    งานวิจัยนี้พัฒนาเว็บไซต์ที่ช่วยให้เราสามารถทำนายประจุย่อยได้
\end{enumerate}

%--------------------------
\section{การทำนายไดโพลโมเมนต์}
\label{sec:pred_dipole_moment}
\idxth{การทำนายคุณสมบัติของโมเลกุล!การทำนายไดโพลโมเมนต์}
\idxen{Molecular Property Prediction!Dipole Moment}
%--------------------------

การทำนายไดโพลโมเมนต์นั้นถือว่าเป็นอีกหนึ่งหัวข้องานวิจัยที่ได้รับความนิยมเพราะว่าไดโพลโมเมนต์นี้เป็นคุณสมบัติที่สำคัญมาก ซึ่งเกี่ยวข้องโดย%
โดยตรงกับเทคนิคทางการทดลอง เช่น Infrared Spectroscopy ดังนั้นการที่ ML เข้ามาช่วยเราให้สามารถทำนายไดโพลโมเมนต์ได้รวดเร็วและ%
ถูกต้องได้นั้นก็จะเป็นการช่วยทำให้เราสามารถศึกษาโมเลกุลได้ง่ายมากยิ่งขึ้นโดยไม่จำเป็นที่จะต้องไปใช้วิธีการคำนวณแบบดั้งเดิมซึ่งมีความซับซ้อน%
เชิงการคำนวณและสิ้นเปลืองเวลาเยอะกว่ามาก\autocite{knijff2021,liu2020,pereira2018,staacke2022,sun2022,veit2020,vo2019}

%--------------------------
\section{การทำนายคุณสมบัติของโมเลกุลที่สถานะกระตุ้น}
\label{sec:pred_ex_prop}
\idxth{การทำนายคุณสมบัติของโมเลกุล!คุณสมบัติของโมเลกุลที่สถานะกระตุ้น}
\idxen{Molecular Property Prediction!Excited State}
%--------------------------

หนึ่งในงานวิจัยที่น่าสนใจที่ตีพิมพ์ในบทความ \enquote{Excited State Non-adiabatic Dynamics of Large Photoswitchable Molecules 
Using a Chemically Transferable Machine Learning Potential}\autocite{axelrod2022} มีงานวิจัยที่ได้ใช้ ML ในการทำนาย 
Quantum Yield\footnote{ผู้เขียนขอใช้ทับศัพท์เพราะถ้าหากช้คำแปลเป็นภาษาไทยจะใช้คำว่า \enquote{ผลผลิตควอนตัม} ซึ่งยากต่อการเข้าใจ 
ดังนั้นจึงขอใช้คำว่า Quantum Yield ตรง ๆ} ของโมเลกุลขนาดใหญ่ ณ สถานะกระตุ้น ซึ่งในงานวิจัยนี้ได้ศึกษาอนุพันธ์ของโมเลกุลเอโซเบนซีน 
(Azobenzene) รวมถึงไปการทรานซิชันระหว่างคอนฟอร์เมอร์ \textit{cis} กับ \textit{trans} อีกด้วย

%--------------------------
\section{การทำนายค่าคู่ควบเชิงเล็กอิเล็กทรอนิกส์}
\label{sec:pred_elec_coupling}
\idxth{การทำนายคุณสมบัติของโมเลกุล!ค่าคู่ควบเชิงเล็กอิเล็กทรอนิกส์}
\idxen{Molecular Property Prediction!Electronic Coupling}
%--------------------------

ค่าคู่ควบเชิงเล็กอิเล็กทรอนิกส์ (Electronic Coupling) คือค่าความเกี่ยวเนื่องเชิงอิเล็กทรอนิกส์ระหว่าง 2 สถานะใด ๆ ของอิเล็กตรอน เช่น 
สถานะเริ่มต้นและสถานะสิ้นสุดในกระบวนการทางควอนตัม 

%--------------------------
\subsection{ค่าคู่ควบของการถ่ายโอนอิเล็กตรอน}
\label{ssec:pred_etran_coupling}
\idxth{การทำนายคุณสมบัติของโมเลกุล!ค่าคู่ควบของการถ่ายโอนอิเล็กตรอน}
\idxen{Molecular Property Prediction!Electron Transfer Coupling}
%--------------------------



%--------------------------
\subsection{ค่าคู่ควบแบบนอนอะเดียแบติก}
\label{ssec:nonadia_coupling}
\idxth{การทำนายคุณสมบัติของโมเลกุล!ค่าคู่ควบแบบนอนอะเดียแบติก}
\idxen{Molecular Property Prediction!Nonadiabatic Coupling}
%--------------------------



%--------------------------
\section{การทำนายสเปกตรัม}
\label{sec:pred_spectra}
\idxth{การทำนายคุณสมบัติของโมเลกุล!การทำนายสเปกตรัม}
\idxen{Molecular Property Prediction!Spectra}
%--------------------------

การทำนายสเปกตรัมเป็นอีกหนึ่งหัวข้องานวิจัยทางด้านเคมีควอนตัมที่ได้รับความสนใจเป็นอย่างมากนั่นก็เพราะว่าเทคนิคทางสเปกโทรสโกปีนั้นมีประโยชน์%
อย่างมากในงานทางด้านเคมีสังเคราะห์ ทั้งเคมีอินทรีย์และเคมีอนินทรีย์ รวมไปถึงด้านอื่น ๆ เช่น วัสดุศาสตร์หรือพอลิเมอร์ด้วย 

สเปกโทรสโกปีเป็นเทคนิคที่เกี่ยวข้องกับแสงซึ่งเป็นคลื่นแม่เหล็กไฟฟ้าที่มีลักษณะเป็นแถบพลังงาน (Spectrum) โดยมีความยาวคลื่นตั้งแต่ในช่วง 
คลื่นวิทยุ คลื่นไมโครเวฟ คลื่นอินฟราเรด คลื่นในช่วงที่สายตามนุษย์มองเห็น รวมถึงคลื่นอัลตราไวโอเลต สำหรับคลื่นแม่เหล็กไฟฟ้าที่นักเคมีสนใจนั้น%
จะเกี่ยวข้องโดยตรงกับการระบุถึงความจำเพาะเจาะจงของโมเลกุล นั่นคือคลื่นแม่เหล็กไฟฟ้าในช่วงอินฟราเรด ซึ่งจะมีความยาวคลื่น (Wavelength) 
ในช่วง 650 - 4,000 nm หรือมีเลขคลื่น (Wavenumber) ในช่วง 14,286-12,800 $cm^{-1}$ โดยหลักการคร่าว ๆ ของเทคนิคอินฟราเรด%
สเปกโทรโกปีก็คือแสงอินฟราเรดตกกระทบโมเลกุลจะเกิดอันตรกิริยาระหว่างแสงกับโมเลกุล โดยที่แสงอินฟราเรดในบางช่วงที่มีความถี่ตรงกันกับความถี่%
ของการสั่นของพันธะในโมเลกุลจะถูกดูดกลืนไป ซึ่งทำให้เกิดทรานซิชันการสั่นพร้อมกับทรานซิชันการหมุน ซึ่งทรานซิชันการสั่นนี้จะเราทราบชนิดของ%
หมู่ฟังก์ชัน เช่น พันธะคู่ พันธะสาม หมู่คาร์บอนิล หมู่ไฮดรอกซิล หรือหมู่อะมิโน ภายในโครงสร้างของโมเลกุลอินทรีย์ได้ ดังนั้นความเข้มของแสง%
อินฟราเรดที่ทะลุผ่านสารตัวอย่าง (Transmitted Infrared) จึงมีความเข้มแสงลดลงในบางช่วงของความถี่ทั้งหมดของอินฟราเรดเนื่องมาจากการ%
ถูกดูดกลืนโดยหมู่ฟังก์ชันดังกล่าวนั่นเอง

%--------------------------
\subsection{การทำนายอินฟราเรดสเปกโทรโกปี}
\label{ssec:pred_spec_ir}
\idxth{การทำนายคุณสมบัติของโมเลกุล!การทำนายสเปกตรัม!อินฟราเรดสเปกโทรโกปี}
\idxen{Molecular Property Prediction!Spectra!Infrared}
%--------------------------

การทำนายสเปกตรัมอินฟราเรดด้วย ML นั้นได้รับการค้นคว้ามาอย่างต่อเนื่องเป็นระยะเวลาหลายสิบปี\autocite{gastegger2017} สำหรับงานวิจัย%
ที่ผู้เขียนจะยกมาเป็นกรณีศึกษานั้นเป็นงานวิจัยที่มีชื่อบทความว่า \enquote{Infrared Spectra at Coupled Cluster Accuracy from Neural 
Network Representations} หรือแปลเป็นภาษาไทยคือ \enquote{การทำนาย IR Spectrum ที่ระดับความแม่นยำเดียวกับระเบียบวิธี CCSD(T) 
ด้วย Neural Network}\autocite{beckmann2022} โดยงานวิจัยนี้ได้รับการตีพิมพ์ในวารสาร Journal of Chemical Theory and Computation 
(JCTC) ซึ่งเป็นวารสารวิชาการแนวหน้าทางด้านเคมีทฤษฎีและการคำนวณทางคอมพิวเตอร์ 

รายละเอียดงานวิจัยในบทความฉบับนี้มีดังนี้ ผู้วิจัยได้สร้าง Neural Network โดยใช้สถาปัตยกรรมโครงสร้างประสาทของ Behler-Parrinello 
Neural Network (BPNN)\autocite{behler2007,behler2011b,behler2015} ซึ่งเป็น Neural Network เชิงโมเลกุลแบบที่มีจำนวนมิติสูง 
(High-dimensional Molecular Neural Network) ที่มีแนวคิดคือผลรวมของพลังงานทั้งหมดของโมเลกุลเกิดขึ้นจากการรวมกันของพลังงาน%
ของแต่ละอะตอม โดยที่ Neural Network ที่ผู้วิจัยได้พัฒนาขึ้นมาได้ถูกนำไปใช้ในการเรียนรู้ (Learn) โครงสร้างของโมเลกุลและ Fit เข้ากับค่า 
Dipole Moment $(\mu)$ ของโมเลกุลนั้น ๆ ซึ่ง $\mu$ คือพารามิเตอร์สำคัญที่เราสามารถนำไปใช้ในการคำนวณหาความเข้มหรือ Intensity 
ของ IR Spectrum (สำหรับแต่ละ Peak) ต่อไปได้

แต่ว่าผู้วิจัยไม่ได้ Fit ข้อมูลเชิงโครงสร้างของโมเลกุลเข้ากับ $\mu$ โดยตรงเพราะว่าพารามิเตอร์ทั้งสองนี้ไม่ได้สอดคล้องหรือเกี่ยวข้องกันโดยตรง 
ดังนั้นเราควรจะต้องทำการ Fit เข้ากับบางสิ่งบางอย่างที่อธิบายเคมีเชิงอิเล็กทรอนิกของโมเลกุลได้ดีกว่าข้อมูลเชิงโครงสร้างทั่วไป ซึ่งสิ่งนั้นเรียกว่า 
Electronic-based Descriptor โดย Descriptor ที่ผู้วิจัยเลือกใช้ก็คือ Atomic-centered Symmetry Function (ACSF) โดยได้ทำการ 
Fit ACSF เข้ากับประจุย่อยของแต่ละอะตอม (Atomic Partial Charge) ซึ่งผู้อ่านอาจจะสงสัยว่าทำไมถึงไม่ Fit ACSF กับ $\mu$ โดยตรงเลย? 
เหตุผลก็เพราะว่า $\mu$ นั้นถูกคำนวณมาจากผลรวมของผลคูณระหว่างประจุ $(q_{i})$ และตำแหน่งของแต่ละอะตอม $(\vec{r}_{i})$ 
ภายในโมเลกุลตามสมการที่ \ref{eq:dipole_moment}

\begin{equation}\label{eq:dipole_moment}
    \mu = \sum^{N}_{i} q_{i}\vec{r}_{i}
\end{equation}

\noindent ดังนั้นจึงจะเหมาะกว่าถ้าเรา Fit เข้ากับประจุก่อน หลังจากนั้นเอาต์พุตสุดท้ายที่ถูกทำนายออกมาจาก Neural Network นั้นก็จะเป็น 
$\mu$ นั่นเอง สรุปลำดับการ Fit ข้อมูลมีดังนี้ 

\fbox{%
\begin{minipage}[c][1cm]{0.9\linewidth}
\centering
Structure $\rightarrow$ ACSF $\rightarrow$ Atomic Partial Chage $\rightarrow$ Dipole Moment 
\end{minipage}}

นอกจากนี้ผู้วิจัยเลือกใช้ RMSE เป็น Lost Function สำหรับการปรับลด Error ระหว่าง $\mu$ ที่ได้จากการทำนายกับค่าอ้างอิงที่ได้จากการ%
คำนวณด้วยวิธี CCSD(T) โดยใช้โปรแกรม Molpro และได้มีการปรับแต่ง Loss Function โดยทำการคำนวณ Error ของทุกคอนฟอร์เมอร์ 
ซึ่งจะทำการปรับค่าลดค่าคลาดเคลื่อนจาก $\mu$ ของทั้งสามทิศทาง (3 Components) นั่นคือ $x$, $y$ และ $z$ ตามสมการดังต่อไปนี้

\begin{equation}
    \mathcal{L} = \frac{1}{3M} \sum^{M}_{i} \sum^{3}_{\alpha} (\mu^{\text{NN}}_{i,\alpha} 
    - \mu^{\text{ref}}_{i,\alpha})^{2}
\end{equation}

\noindent โดยที่ $M$ คือจำนวนคอนฟอร์เมอร์ และ $\alpha$ คือทิศทาง หลังจากนั้นผู้วิจัยใช้ Autocorrelation Function ในการแปลง 
$\mu$ (เปลี่ยนจาก Time-domain ไปเป็น Frequency-domain) เพื่อคำนวณหาสเปกตรัมของอินฟราเรดต่อไป 

สำหรับชุดโมเลกุลที่ผู้วิจัยศึกษาในงานนี้เป็นแค่กลุ่มโมเลกุลง่าย ๆ คือกลุ่มโมเลกุลน้ำ (Water Cluster) โดยมีการศึกษาความสามารถในการ%
เรียนรู้ของโมเดลที่เปลี่ยนแปลงไปตามขนาดของชุดข้อมูลฝึกสอนและชุดข้อมูลทดสอบ โดยผู้วิจัยได้มีการใช้ชุดข้อมูลฝึกสอนขนาดเล็กสุดคือ 5,975 
โครงสร้างและใหญ่สุดคือ 18,576 โครงสร้าง ซึ่งประสิทธิภาพในการทำนายถือว่าอยู่ในระดับที่แม่นยำมาก โดยมีค่าคลาดเคลื่อนการทำนาย $\mu$ 
ต่อโมเลกุลอยู่ที่ 0.007 D และต่ออะตอมอยู่ที่ประมาณ 0.002 D

%--------------------------
\subsection{การทำนายรามานสเปกโทรโกปี}
\label{ssec:pred_spec_raman}
\idxth{การทำนายคุณสมบัติของโมเลกุล!การทำนายสเปกตรัม!รามานสเปกโทรโกปี}
\idxen{Molecular Property Prediction!Spectra!Raman}
%--------------------------


%--------------------------
\subsection{วิธีการทำนายสเปกตรัมและโครงสร้าง}
\label{ssec:pred_spec_struct}
\idxth{การทำนายคุณสมบัติของโมเลกุล!การทำนายสเปกตรัม!วิธีการทำนายสเปกตรัมและโครงสร้าง}
\idxen{Molecular Property Prediction!Spectra!Predicting Spectra and Structures}
%--------------------------

\begin{table}[H]
    \centering
    \caption{บทความงานวิจัยที่เกี่ยวข้องกับวิธี Struc-to-Spec}
    \label{tab:struc2spec}
    \small
    \begin{tabular}{clll}
    \toprule
    \textbf{อ้างอิง} &\textbf{Learning Target} &\textbf{โมเดล ML} &\textbf{Representation} \\
    \midrule
    \autocite{schuur1996,Schuur1997} & ความเข้มการดูดกลืน IR & PCA+CPG\autocite{hecht-nielsen1987}, 
    CPG\autocite{hecht-nielsen1987} & คุณสมบัติเชิงอะตอม, ระยะห่างระหว่างอะตอม \\
    
    \autocite{selzer2000,kostka2001} & ความเข้มการดูดกลืน IR & \makecell[tl]{Query Driven Selection \\ + RDF + 
    CPG\autocite{hecht-nielsen1987}} & คุณสมบัติเชิงอะตอม, ระยะห่างระหว่างอะตอม \\
    
    \autocite{yildiz2011,yildiz2012} & ความเข้ม IR/Raman & LFFN-EPFs\autocite{yildiz2011} & ความเข้ม IR/Raman \\
    
    \autocite{gastegger2017} & ประจุย่อยเชิงอะตอม & HDNNP\autocite{behler2007} + ED-GEKF\autocite{gastegger2015} 
    & ACSFs\autocite{behler2011a}, Geometric Descriptors \\
    
    \autocite{sifain2018,nebgen2018} & ประจุย่อยเชิงอะตอม & HIP-NN\autocite{lubbers2018} & เลขอะตอม, 
    ระยะห่างระหว่างอะตอม \\
   \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{บทความงานวิจัยที่เกี่ยวข้องกับวิธี Spec-to-Struc}
    \label{tab:spec2struc}
    \small
    \begin{tabular}{clll}
    \toprule
    \textbf{อ้างอิง} &\textbf{Learning Target} &\textbf{โมเดล ML} &\textbf{Representation} \\
    \midrule
    \autocite{visser1994,luinge1995} & โครงสร้างและหมู่ฟังก์ชัน & 
    PLS\autocite{wold1984}, PCA-MLP, MLP & \makecell[tl]{Band Pattern \\ Full Spectrum} \\

    \autocite{carrieri1995} & โมเลกุลทั่วไป & Binary และ Decimal-based NN & 
    Band Pattern \\
    
    \autocite{fu2018} & คอนฟิกูเรชั่นเชิงนิวเคลียร์ & Basin-hopping Algorithm\autocite{wales1997} & 
    Cosine Distances \\
    
    \autocite{fine2020} & หมู่ฟังก์ชัน & Autoencoder + MLP, RF\autocite{breiman2001} & 
    Peak ของ FTIR \\
    
    \autocite{ren2021} & โครงสร้างที่มีหมู่ \ce{OH} และ \ce{C=O} & LSTM \autocite{hochreiter1997} & 
    \makecell[tl]{Peak ของ IR และ Raman \\ ที่คำนวณด้วย DFT} \\
   \bottomrule
\end{tabular}
\end{table}

โดยตัวย่อที่ใช้ในตารางที่ \ref{tab:struc2spec} และ \ref{tab:spec2struc} มีชื่อเต็มดังต่อไปนี้

\begin{itemize}[topsep=0pt,noitemsep]
    \item CPG: Counterpropagation
    
    \item ED-GEKF: Element-decoupled Global Extended Kalman Filter
    
    \item FTIR: Fourier Transform Infrared
    
    \item HDNNP: High Dimensional Neural Network Potential
    
    \item HIP-NN: Hierarchically Interacting Particle Neural Network
    
    \item LFFN-EPFs: Layered Feed Forward Neural Network - Empirical Physical Formulas
    
    \item MLP: Multilayer Perceptron
    
    \item NMA: N-methylacetamide
    
    \item PCA: Principal Component Analysis
    
    \item PG-EA: Probability Graph-evolutionary Algorithm
    
    \item RDF: Radial Distribution Function
\end{itemize}

\idxen{Molecular Property Prediction!Spectra!Struc-to-Spec}
\idxen{Molecular Property Prediction!Spectra!Spec-to-Struc}

%--------------------------
\section{บทความวิชาการเพิ่มเติม}
\label{sec:pred_misc_papers}
%--------------------------

นอกเหนือจากการนำ ML ไปใช้สำหรับการทำนายพารามิเตอร์ต่าง ๆ แล้ว ถ้าหากผู้อ่านสนใจการประยุกต์ใช้ ML กับงานทางด้านอื่น ๆ ของเคมีควอนตัม 
สามารถอ่านบทความวิชาการเพิ่มเติมได้จากวารสารวิชาการชั้นแนวหน้า เช่น 

\begin{enumerate}[topsep=0pt,noitemsep]
    \item Journal of Chemical Information and Modeling (\textit{J. Chem. Inf. Model}) 
    
    \item Journal of Chemical Physics (\textit{J. Chem. Phys.}) 
    
    \item Journal of Chemical Theory and Computation (\textit{J. Chem. Theory Comput.}) 
    
    \item Journal of Physical Chemistry A (\textit{J. Phys. Chem. A}) 
    
    \item Journal of Physical Chemistry Letters (\textit{J. Phys. Chem. Lett.}) 
    
    \item Machine Learning: Science and Technology (\textit{MLST}) 
    
    \item Nature Machine Intelligence (\textit{Nat. Mach. Intell}) 
    
    \item Nature Communications (\textit{Nat. Commun.})
    
    \item Proceedings of the National Academy of Sciences of the United States of America (\textit{PNAS}) 
    
    \item Science Advances (\textit{Sci. Adv.}) 
\end{enumerate}

\noindent โดยวารสารอันดับที่ 1-5 เป็นวารสารเฉพาะทางด้านเคมีทฤษฎีและเคมีคอมพิวเตอร์, วารสารอันดับที่ 6-7 เป็นวารสารเฉพาะทางด้าน%
การเรียนรู้ของเครื่อง, และวารสารอันดับที่ 8-10 เป็นวารสารด้านวิทยาศาสตร์ทั่วไป นอกจากนี้แล้วยังมีวารสารอื่น ๆ อีกที่ไม่ได้กล่าวถึงรวมไปถึง
Repository ที่ให้บริการเผยแพร่บทความงานวิจัยแบบเข้าถึงได้ฟรี (Open-access) เช่น arXiv (\url{https://arxiv.org/}) และ 
ChemRxiv (\url{https://chemrxiv.org})

%--------------------------
\subsection{บทความเฉพาะทาง}
\label{ssec:pred_misc_papers_specific}
%--------------------------

โดยผู้เขียนได้เลือกงานวิจัยที่มีความโดดเด่นและเหมาะสำหรับผู้เริ่มต้นศึกษา ML และเคมีควอนตัม ซึ่งน่าจะช่วยให้ผู้อ่านเห็นภาพรวมของโจทย์งานวิจัย%
ในปัจจุบันที่กำลังมาแรง บทความที่คัดเลือกมาประกอบไปด้วยบทความการทบทวนงานวิจัย (Review) ที่ใช้ ML ในการเรียนรู้ Force Field 
สำหรับงานทางด้านเคมีควอนตัมและการจําลองพลวัตเชิงโมเลกุล (QM/MD) หรือนำมาใช้ในการทำนายพื้นที่พลังงานอิสระ (Free Energy Landscape)
ไปจนถึงการพัฒนาโมเดล ML เพื่อทำนายคุณสมบัติเชิงโมเลกุล เช่น ไดโพลโมเมนต์ (Dipole Moment) และสภาพการเกิดขึ้น (Polarizability)

\begin{enumerate}[noitemsep]
    \item \enquote{PhysNet: A Neural Network for Predicting Energies, Forces, Dipole Moments, and 
    Partial Charges}\autocite{unke2019}\\
    ตีพิมพ์เมื่อวันที่ 01 พฤษภาคม ค.ศ. 2019
    
    \item \enquote{Comparison of the Performance of Machine Learning Models in Representing High-Dimensional 
    Free Energy Surfaces and Generating Observables}\autocite{cendagorta2020}\\
    ตีพิมพ์เมื่อวันที่ 10 เมษายน ค.ศ. 2020
    
    \item \enquote{Kernel-Based Machine Learning for Efficient Simulations of Molecular Liquids}\autocite{scherer2020}\\
    ตีพิมพ์เมื่อวันที่ 13 เมษายน ค.ศ. 2020

    \item \enquote{Machine Learning Force Fields}\autocite{unke2021}\\
    ตีพิมพ์เมื่อวันที่ 11 มีนาคม ค.ศ. 2021

    \item \enquote{The Rise of Neural Networks for Materials and Chemical Dynamics}\autocite{kulichenko2021}\\
    ตีพิมพ์เมื่อวันที่ 1 กรกฎาคม ค.ศ. 2021
\end{enumerate}

%--------------------------
\subsection{บทความรีวิว}
\label{ssec:pred_misc_papers_review}
%--------------------------

ในช่วง 5 ปีที่ผ่านมานี้ (ค.ศ. 2017-2022) นอกจากจะมีการตีพิมพ์บทความงานวิจัยเฉพาะทางออกมาอย่างต่อเนื่องแล้ว ยังได้มีการตีพิมพ์บทความ%
วิจารณ์หรือรีวิวซึ่งเป็นการสรุปภาพรวมของการใช้ ML ในหัวข้อต่าง ๆ ของเคมีทฤษฎีซึ่งผู้เขียนได้สรุปไว้ในตารางที่ \ref{tab:review_ml_chem}

\begin{table}[H]
    \centering
    \caption{บทความรีวิวงานวิจัยทางด้าน ML และเคมีทฤษฏี}
    \label{tab:review_ml_chem}
    \begin{tabular}{cll}
    \toprule
    \textbf{ปี ค.ศ. ที่ตีพิมพ์} &\textbf{ผู้แต่งและบทความอ้างอิง}\footnotetext{ผู้แต่งในที่นี้คือนามสกุลของผู้แต่ง} 
    &\textbf{หัวข้อที่รีวิว} \\
    \midrule
    2017 &Behler\autocite{behler2017} &พลังงานศักย์ระหว่างอะตอม \\
    2018 &Goldsmith และคณะ\autocite{goldsmith2018} &ML สำหรับ Catalysis \\
    2019 &Carleo และคณะ\autocite{carleo2019} &ML สำหรับวิทยาศาสตร์เชิงกายภาพ \\
    2019 &Yang และคณะ\autocite{yang2019} &Drug Discovery \\
    2019 &Elton และคณะ\autocite{elton2019} &Molecular Design \\
    2019 &Schleder และคณะ\autocite{schleder2019} &ML สำหรับวัสดุศาสตร์ \\
    2019 &Ceriotti\autocite{ceriotti2019} &การเรียนรู้แบบไม่มีผู้สอน \\
    2020 &Dral\autocite{dral2020a} &ML สำหรับเคมีควอนตัม \\
    2020 &No\'{e} และคณะ\autocite{noe2020} &การจำลองเชิงโมเลกุล \\
    2020 &von Lilienfeld และคณะ\autocite{vonlilienfeld2020} &ปริภูมิเคมี \\
    2020 &Mueller และคณะ\autocite{mueller2020} &พลังงานศักย์ระหว่างอะตอม \\
    2020 &Manzhos และคณะ\autocite{manzhos2021} &ML สำหรับโมเลกุลและปฏิกิริยาขนาดเล็ก \\
    2020 &Gkeka และคณะ\autocite{gkeka2020} &Force Fields และ Coarse Graining \\
    2020 &Unke และคณะ\autocite{unke2021} &Force Fields \\
    2020 &Toyao และคณะ\autocite{toyao2020} &การเร่งปฏิกิริยาเชิงข้อมูล \\
    2020 &Manzhos\autocite{manzhos2020} &ML สำหรับโครงสร้างเชิงอิเล็กทรอนิกส์ \\
    2020 &\makecell[tl]{Westermayr และ \\Marquetand\autocite{westermayr2021a}} &ML สำหรับสถานะกระตุ้น \\
    2021 &Behler\autocite{behler2021} &Neural Network Potentials \\
    2021 &Westermayr และคณะ\autocite{westermayr2021b} &ML สำหรับเคมีเชิงคำนวณและวัสดุศาสตร์ \\
    2021 &Zheng และคณะ\autocite{zheng2021} &วิธีทางควอนตัมและการทำนายพลังงาน \\
    2022 &Sajjan และคณะ\autocite{sajjan2022} &การพัฒนา ML สำหรับ Quantum Computing \\
    2022 &Lim และคณะ\autocite{lim2022} &คุณสมบัติของโมเลกุลและ Drug Discovery \\
    2022 &Raghunathan และคณะ\autocite{raghunathan2022} &Representation สำหรับ ML \\
    2022 &Stuyver และคณะ\autocite{stuyver2022} &ML สำหรับความว่องไวของปฏิกิริยาเคมี \\
    2022 &Tavakoli และคณะ\autocite{tavakoli2022} &ML (Graph) สำหรับความว่องไวของปฏิกิริยาเคมี \\
    2022 &Han และคณะ\autocite{han2022} &ML สำหรับสเปกโทรสโกปี \\
    2022 &van Gerwen และคณะ\autocite{gerwen2022} &Representation สำหรับ Chemical Reactivity \\
    2022 &Qiao และคณะ\autocite{qiao2022} &Geometric Deep Learning \\
    \bottomrule
    \end{tabular}
\end{table}

นอกจากนี้ยังมีบทความรีวิวที่ผู้เขียนแนะนำให้ผู้สนใจศึกษา ML สำหรับเคมีควอนตัมแบบเชิงลึกโดยเฉพาะการต่อยอดในงานวิจัย ดังนี้

\begin{enumerate}
    \item \enquote{Roadmap on Machine Learning in Electronic Structure}\autocite{kulik2022}\\
    อธิบายภาพรวมของ ML กับการศึกษาโครงสร้างเชิงอิเล็กทรอนิกส์ของโมเลกุล
    
    \item \enquote{Unsupervised Learning Methods for Molecular Simulation Data}\autocite{glielmo2021}\\
    อธิบายการใช้เทคนิคการเรียนรู้แบบไม่มีผู้สอนสำหรับการจำลองและการวิเคราะห์ข้อมูลเชิงโมเลกุล

    \item \enquote{Physics-Inspired Structural Representations for Molecules and Materials}\autocite{musil2021}\\
    อธิบาย Representations สำหรับโมเลกุลและวัสดุ

    \item \enquote{Combining Machine Learning and Computational Chemistry for Predictive Insights Into Chemical 
    Systems}\autocite{keith2021}\\
    อธิบายการใช้ ML ในการทำนายคุณสมบัติเชิงเคมี

    \item \enquote{Machine Learning for Electronically Excited States of Molecules}\autocite{westermayr2021a}\\
    อธิบาย ML สำหรับการศึกษาโมเลกุลในสถานะกระตุ้น

    \item \enquote{Ab Initio Machine Learning in Chemical Compound Space}\autocite{huang2021}\\
    อธิบายการใช้ ML สำหรับการศึกษาปริภูมิเคมีขนาดใหญ่

    \item \enquote{Four Generations of High-Dimensional Neural Network Potentials}\autocite{behler2021}\\
    อธิบายโครงข่ายประสาทสำหรับการทำนายพลังงานของโมเลกุลทั้ง 4 รุ่น

    \item \enquote{Gaussian Process Regression for Materials and Molecules}\autocite{deringer2021}\\
    อธิบายการใช้เทคนิค GPR ในการทำนายคุณสมบัติของโมเลกุลและวัสดุ

    \item \enquote{Machine Learning Force Fields}\autocite{unke2021}\\
    อธิบายการเรียนรู้และสร้าง Force Field โดยใช้ ML

    \item \enquote{Neural Network Potential Energy Surfaces for Small Molecules and Reactions}\autocite{manzhos2021}\\
    อธิบายการศึกษาโมเลกุลและปฏิกิริยาเคมีระบบเล็กโดยใช้โครงข่ายประสาทของพื้นผิวพลังงานศักย์

    \item \enquote{Machine Learning for Chemical Reactions}\autocite{meuwly2021}\\
    อธิบายการใช้ ML สำหรับทำนายปฏิกิริยาเคมี
\end{enumerate}

% LaTeX source for ``การเรียนรู้ของเครื่องสำหรับเคมีควอนตัม (Machine Learning for Quantum Chemistry)''
% Copyright (c) 2022 รังสิมันต์ เกษแก้ว (Rangsiman Ketkaew).

% License: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)
% https://creativecommons.org/licenses/by-nc-nd/4.0/

\chapter{การเรียนรู้แบบมีผู้สอน}
\label{ch:sup_ml}

การเรียนรู้แบบมีผู้สอนหรือ Supervised learning เป็นเทคนิคแรก ๆ ที่ถูกพัฒนาขึ้นมาในช่วงยุคเริ่มต้นของ ML ซึ่งเป็นแนวคิดที่ใช้ Input และ 
Output ในการเทรนโมเดล ซึ่งผู้เขียนมีความคิดเห็นส่วนตัวว่าการสร้าง Model ประเภทนี้จะง่าย ๆ ที่สุดทั้งในแง่ทฤษฎี การเรียนรู้ และการนำไปใช้ 
โดยเทคนิคนี้ได้รับความนิยมมากที่สุดนั่นก็เพราะว่าสามารถนำไปประยุกต์ใช้งานกับโจทย์ที่หลากหลาย

%--------------------------
\section{การถดถอยเชิงเส้น}
\label{sec:lin_res}
\idxen{Supervised Learning!Linear Regression}
%--------------------------

เทคนิคของการเรียนรู้แบบมีผู้สอนที่พื้นฐานที่สุดและได้รับความนิยมอย่างมากในช่วงยุคแรกของปัญญาประดิษฐ์ก็คือการถดถอยแบบเชิงเส้น (Linear Regression)
สมมติว่าเราพิจารณาชุดข้อมูลที่มีตัวแปรต้น 2 ตัว ($x_{1}$ และ $x_{2}$) และมีตัวแปรตาม 1 ตัว (y) ซึ่งตัวแปรตามในที่นี้ก็คือคำตอบหรือเป้าหมายที่%
เราต้องการทำนายนั่นเอง โดยยกตัวอย่างเช่น กำหนดให้ $x_{1}$ คเป็นจำนวนพันธะเดี่ยวในโมเลกุล $x_{2}$ เป็นจำนวนวงอะโรมาติก (Aromatic)
ในโมเลกุล และ $y$ เป็นค่าพลังงานรวมของโมเลกุล เราพบว่าเราสามารถสร้างหรือกำหนดสมการที่อธิบายความสัมพันธ์ระหว่างตัวแปรทั้งสามตัวนี้ได้%
แบบง่าย ๆ ดังนี้

\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2
\end{equation}

\noindent โดยที่ $x$ ในที่นี้คือเวกเตอร์แบบสองมิติในปริภูมิ $\mathbb{R}^{2}$ และ $\theta_{i}$ คือพารามิเตอร์หรือเรียกว่าน้ำหนัก 
(Weights) ก็ได้ ซึ่งจะเป็นตัวแปรที่ปรับความเชื่อมโยง (Mapping) ระหว่าง $x_{i}$ และ $y$ ซึ่งเราสามารถเขียนให้อยู่ในรูปทั่วไปได้ดังนี้

\begin{align}
    h(x) &= \sum_{i=0}^{d} \theta_{i} x_{i} \\
         &= \theta^{\top} x
\end{align}

\noindent โดยสมการด้านบนนั้นจะเขียนในรูปของผลคูณระหว่างเวกเตอร์ของพารามิเตอร์ ($\theta^{\top}$) และเวกเตอร์ $x$

ลำดับถัดมาคือเราจะทำการปรับพารามิเตอร์ $\theta$ อย่างไรเพื่อให้ได้ชุดพารามิเตอร์ที่ทำการ Mapping ได้ดีที่สุด คำตอบก็คือเราสามารถทำได้โดย%
การกำหนดฟังก์ชันที่จะเป็นตัววัดพารามิเตอร์ $\theta_{i}$ ทีละตัว ซึ่งเรากำหนดและเรียกฟังก์ชันที่จะมาช่วยเราว่า Cost Function (Loss Function)
โดยมีรูปสมการทั่วไปดังต่อไปนี้ 

\begin{equation}
    J(\theta) = \frac 1 2 \sum_{i=1}^n \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
\end{equation}

\noindent ซึ่งจะมีความคล้ายกันกับ Ordinary Least Square นั่นเอง โดยในหัวข้อต่อไปเราจะมาดูรายละเอียดของเทคนิคที่เราสามารถนำมาใช้%
ในการแก้ปัญหาของ Coss Function

เสริม: สำหรับฟังก์ชันที่มีความเป็นเชิงเส้นนั้นจะต้องสอดคล้องกับเงื่อนไขดังต่อไปนี้

\begin{equation}
    f(\vec{x} + \vec{y}) = f(\vec{x}) + f(\vec{y})
\end{equation}

\noindent สำหรับ $\vec{x}$ และ $\vec{y}$ ทุกค่า และเงื่อนไขที่สองคือ

\begin{equation}
    f(s\vec{x}) = sf(x)
\end{equation}

\noindent ถ้าหากฟังก์ชันไม่สอดคล้องกับเงื่อนไขทั้งสองข้อด้านบน ฟังก์ชันนั้นจะมีความไม่เป็นเชิงเส้น (Nonlinearity)

%--------------------------
\section{การจำแนกประเภท}
\label{sec:classification}
\idxen{Supervised Learning!Classification}
%--------------------------

ในหัวข้อนี้จะเป็นการศึกษาโจทย์ปัญหาแบบการจำแนกประเภท (Classification) ซึ่งก็คล้าย ๆ กับโจทย์แบบ Regression แต่ว่าจะต่างกันตรงที่ค่า
$y$ ที่เราต้องการทำนายนั้นจะมีความไม่ต่อเนื่อง (Discrete Data) ซึ่งจะตรงข้ามกับ Regression ที่ค่า $y$ จะมีความต่อเนื่อง (Continuous Data) 
โดยเริ่มต้นเราจะสนใจกรณี Classification แบบง่ายก่อน นั่นก็คือมีประเภทของข้อมูลที่เราจะจำแนกเพียงแค่ 2 ประเภท เรียกว่าโจทย์ปัญหา 
Binary Classification ซึ่งค่า $y$ จะมีค่าได้แค่ 0 กับ 1 เท่านั้น ซึ่งในภายหลังเราจึงค่อยมาพิจารณากรณีที่มีประเภทมากกว่า 2 ประเภท
(Multiple-class Case) 

สำหรับการระบุชื่อของประเภทหรือคลาส (Class) นั้น เราจะเรียกคลาส 0 ว่าเป็น Negative Class และเรียกคลาส 1 ว่า Positive Class
ซึ่งบ่อยครั้งเรามักจะเจอการใช้เครื่องหมาย - และ + แทนการเขียน 0 กับ 1 โดยที่เราจะกำหนดให้ $y^{i}$ คือ Label สำหรับตัวอย่างการฝึกสอน

%--------------------------
\section{การถดถอยแบบโลจิสติค}
\label{sec:logis_regress}
\idxen{Supervised Learning!Logistic Regression}
%--------------------------

การวิเคราะห์การถดถอยโลจิสติค (Logistic Regression) เป็นการวิเคราะห์ที่มีเป้าหมายเพื่อทำนายโอกาสที่จะเกิดเหตุการณ์ที่สนใจโดยอาศัย%
สมการโลจิสติคที่สร้างขึ้นจากชุดตัวแปรทำนายที่เป็นตัวแปรที่มีข้อมูลอยู่ในระดับช่วงเป็นอย่างน้อย โดยที่ระหว่างตัวแปรทำนายจะต้องมีความสัมพันธ์กันต่ำ 
และในการวิเคราะห์จะต้องใช้ขนาดตัวแปรทำนายไม่ต่ำกว่า 30 ตัวแปร

%--------------------------
\section{เทคนิคอื่น ๆ}
\idxen{Machine Learning Techniques}
%--------------------------

%--------------------------
\subsection{Partial Least Squares (PLS)}
\label{sec:pls}
\idxen{Machine Learning Techniques!Partial Least Squares}
%--------------------------

วิธีกำลังสองน้อยที่สุดบางส่วน (Partial Least Squares หรือ PLS) เป็นวิธีเชิงสถิติที่ใช้สำหรับการวิเคราะห์หลายตัวแปรเพื่อสร้างตัวแบบ%
ความสัมพันธ์ระหว่างกลุ่มของตัวแปรทำนาย (Predictor Variable) โดยอาศัยตัวแปรแฝง (Latent variable) ซึ่งเทคนิคนี้มีความคล้ายกับ 
Principle Component Analysis (PCA) ซึ่งจะเป็นการลดจำนวนมิติของข้อมูล\cite{wold1984} ในช่วงยุคเริ่มต้นที่มีการใช้ปัญญาประดิษฐ์%
ในงานด้านเคมีนั้น เทคนิคนี้ได้ถูกนำมาใช้อย่างแพร่หลาย เช่น นำมาใช้สำหรับการระบุ Vibrational Bands สำหรับ Vibrational Spectra 
และนำผลที่ได้มาเปรียบเทียบกับค่าการทำนายที่ได้จากวิธีอื่น เช่น ANN และ PCA-ANN

%--------------------------
\subsection{Gaussian Process Regression (GPR)}
\label{sec:gpr}
\idxen{Machine Learning Techniques!Gaussian Process Regression}
%--------------------------

การถดถอยของกระบวนการเกาส์เซียน (Gaussian Process Regression หรือ GPR) เป็นวิธีการถดถอยของเบส์แบบหนึ่งโดยใช้ Kernel Function 
ที่สามารถบ่งบอกหรือแสดงค่าความแปรปรวน (Covariance) ในขั้นตอน Gaussian Process ได้\cite{rasmussen2005} โดย GPR 
จะทำการสร้างโมเดลแบบ Non-parametric และสามารถคำนวณค่าความเชื่อมั่น (Confidence intervals) ไปพร้อม ๆ กับการทำนาย 
รายละเอียดเพิ่มเติมของ GPR สามารถศึกษาได้ในหัวข้อ\ref{sec:gaussian_process} ของบท \ref{ch:kernel}

%--------------------------
\subsection{Random Forest}
\label{sec:rs}
\idxen{Machine Learning Techniques!Random Forest}
%--------------------------

Random Forest (RF) เป็นวิธีหนึ่งในกลุ่มของโมเดลที่เรียกว่า Ensemble Learning (การเรียนรู้แบบกลุ่มก้อน) 
ที่มีหลักการคือการเทรนโมเดลที่เหมือนกันหลาย ๆ ครั้ง (Multitude) บนข้อมูลชุดเดียวกัน โดยแต่ละครั้งของการเทรนจะเลือกส่วนของข้อมูลที่%
เทรนไม่เหมือนกัน แล้วนำการตัดสินใจของโมเดลเหล่านั้นมาโหวตกันว่า Class ไหนถูกเลือกมากที่สุด\cite{breiman2001,quinlan1986}

%--------------------------
\subsection{Artificial Neural Network}
\label{sec:ann}
\idxen{Machine Learning Techniques!Artificial Neural Network}
%--------------------------

โครงข่ายประสาทเทียม (Artificial Neural Network หรือ ANN) เป็นอัลกอริทึมรูปแบบหนึ่งที่เลียนแบบการทำงานของสมองมนุษย์
โดยทำการสร้างโมเดลเรียนรู้ที่ประกอบไปด้วยชั้นเรียนรู้ระหว่างกลาง (Hidden Layer) และหน่วยย่อยที่เกิดการเรียนรู้ (Node หรือ Artificial 
Neuron หรือ Unit)

โดยโมเดลโครงข่ายประสาทเทียมที่มีการนำไปใช้มากที่สุดคือเครือข่ายประสาทแบบป้อนไปหน้า (Feedforward Network) และโมเดล ANN 
ยังสามารถแบ่งออกได้เป็นหลายประเภท ดังนี้

\begin{itemize}
    \item เพอร์เซ็ปตรอนชั้นเดียว (Single-layer Perceptron)
    \item เพอร์เซ็ปตรอนหลายชั้น (Multi-layer Perceptron
    \item โครงข่ายแบบวนซ้ำ (Recurrent Network)
    \item แผนผังจัดระเบียบเองได้ (Self-organizing Map)
    \item เครื่องจักรโบลทซ์แมน (Boltzmann Machine)
    \item กลไกแบบคณะกรรมการ (Committee of Machines)
    \item โครงข่ายความสัมพันธ์ (Associative Neural Network)
    \item โครงข่ายกึ่งสำเร็จรูป (Instantaneously Trained Networks)
    \item โครงข่ายแบบยิงกระตุ้น (Spiking Neural Networks) 
\end{itemize}

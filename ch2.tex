% LaTeX source for ``การเรียนรู้ของเครื่องสำหรับเคมีควอนตัม (Machine Learning for Quantum Chemistry)''
% Copyright (c) 2022 รังสิมันต์ เกษแก้ว (Rangsiman Ketkaew).

% License: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)
% https://creativecommons.org/licenses/by-nc-nd/4.0/

\chapter{การเรียนรู้แบบมีผู้สอน}
\label{ch:sup_ml}

การเรียนรู้แบบมีผู้สอนหรือ Supervised learning เป็นเทคนิคแรก ๆ ที่ถูกพัฒนาขึ้นมาในช่วงยุคเริ่มต้นของการเรียนรู้ของเครื่องซึ่งเป็นแนวคิดที่ใช้%
อินพุตและเอาต์พุตในการเทรนโมเดล ซึ่งผู้เขียนมีความคิดเห็นส่วนตัวว่าการสร้างโมเดลประเภทนี้จะง่าย ๆ ที่สุดทั้งในแง่ทฤษฎี การเรียนรู้ และการนำไปใช้ 
โดยเทคนิคนี้ได้รับความนิยมมากที่สุดนั่นก็เพราะว่าสามารถนำไปประยุกต์ใช้งานกับโจทย์ที่หลากหลาย

%--------------------------
\section{การถดถอยเชิงเส้น}
\label{sec:lin_res}
\idxen{Supervised Learning!Linear Regression}
%--------------------------

เทคนิคของการเรียนรู้แบบมีผู้สอนที่พื้นฐานที่สุดและได้รับความนิยมอย่างมากในช่วงยุคแรกของปัญญาประดิษฐ์ก็คือ การถดถอยแบบเชิงเส้น 
(Linear Regression) สมมติว่าเราพิจารณาชุดข้อมูลที่มีตัวแปรต้น 2 ตัว ($x_{1}$ และ $x_{2}$) และมีตัวแปรตาม 1 ตัว ($y$) 
ซึ่งตัวแปรตามในที่นี้ก็คือคำตอบหรือเป้าหมายที่เราต้องการทำนายนั่นเอง โดยยกตัวอย่างเช่น กำหนดให้ $x_{1}$ เป็นจำนวนพันธะเดี่ยวในโมเลกุล 
$x_{2}$ เป็นจำนวนวงอะโรมาติก (Aromatic) ในโมเลกุล และ $y$ เป็นค่าพลังงานรวมของโมเลกุล เราพบว่าเราสามารถสร้างหรือกำหนดสมการ%
ที่อธิบายความสัมพันธ์ระหว่างตัวแปรทั้งสามตัวนี้ได้แบบง่าย ๆ ดังนี้

\begin{equation}
    h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2
\end{equation}

\noindent โดยที่ $x$ ในที่นี้คือเวกเตอร์แบบสองมิติในปริภูมิ $\mathbb{R}^{2}$ และ $\theta_{i}$ คือพารามิเตอร์หรือเรียกว่าน้ำหนัก 
(Weights) ก็ได้ ซึ่งจะเป็นตัวแปรที่ปรับความเชื่อมโยง (Mapping) ระหว่าง $x_{i}$ และ $y$ ซึ่งเราสามารถเขียนให้อยู่ในรูปทั่วไปได้ดังนี้

\begin{align}
    h(x) &= \sum_{i=0}^{d} \theta_{i} x_{i} \\
         &= \theta^{\top} x
\end{align}

\noindent โดยสมการด้านบนนั้นจะเขียนในรูปของผลคูณระหว่างเวกเตอร์ของพารามิเตอร์ ($\theta^{\top}$) และเวกเตอร์ $x$

ลำดับถัดมาคือเราจะทำการปรับพารามิเตอร์ $\theta$ อย่างไรเพื่อให้ได้ชุดพารามิเตอร์ที่ทำการ Mapping ได้ดีที่สุด คำตอบก็คือเราสามารถทำได้โดย%
การกำหนดฟังก์ชันที่จะเป็นตัววัดพารามิเตอร์ $\theta_{i}$ ทีละตัว ซึ่งเรากำหนดและเรียกฟังก์ชันที่จะมาช่วยเราว่า Cost Function (Loss Function)
โดยมีรูปสมการทั่วไปดังต่อไปนี้ 

\begin{equation}
    J(\theta) = \frac 1 2 \sum_{i=1}^n \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
\end{equation}

\noindent ซึ่งจะมีความคล้ายกันกับ Ordinary Least Square นั่นเอง โดยในหัวข้อต่อไปเราจะมาดูรายละเอียดของเทคนิคที่เราสามารถนำมาใช้%
ในการแก้ปัญหาของ Cost Function

\noindent เสริม: สำหรับฟังก์ชันที่มีความเป็นเชิงเส้นนั้นจะต้องสอดคล้องกับเงื่อนไขดังต่อไปนี้

\begin{equation}
    f(\vec{x} + \vec{y}) = f(\vec{x}) + f(\vec{y})
\end{equation}

\noindent สำหรับ $\vec{x}$ และ $\vec{y}$ ทุกค่า และเงื่อนไขที่สองคือ

\begin{equation}
    f(s\vec{x}) = sf(x)
\end{equation}

\noindent ถ้าหากฟังก์ชันไม่สอดคล้องกับเงื่อนไขทั้งสองข้อด้านบน ฟังก์ชันนั้นจะมีความไม่เป็นเชิงเส้น (Nonlinearity)

%--------------------------
\subsection{การถดถอยแบบง่าย}
\label{sec:simple_lin_res}
\idxen{Supervised Learning!Simple Regression}
%--------------------------

เรามาดูตัวอย่างของกรณีแรกของการถดถอย นั่นก็คือการถดถอยแบบง่าย (Simple Regression) โดยพิจารณาข้อมูลในตาราง 
\ref{tab:simple_reg_data} ดังต่อไปนี้ 

\begin{table}[H]
    \centering
    \caption{แสดงเงินที่ใช้ในการลงทุนการโฆษณาของบริษัทต่าง ๆ กับยอดขายรายปี}
    \label{tab:simple_reg_data}
    \small
    \begin{tabular}{lcc}\toprule
    \textbf{บริษัท} &\textbf{วิทยุ} &\textbf{ยอดขาย (ต่อหน่วย)} \\\midrule
    Amazon &37.8 &22.1 \\
    Google &39.3 &10.4 \\
    Facebook &45.9 &18.3 \\
    Apple &41.3 &18.5 \\
    \bottomrule
    \end{tabular}
\end{table}

ตาราง \ref{tab:simple_reg_data} แสดงความสัมพันธ์ระหว่างเงินที่ใช้ในการลงทุนในสื่อวิทยุของบริษัทต่าง ๆ กับยอดขายรายปีต่อหน่วยการลงทุน 
โดยเราสามารถกำหนดตัวแปรได้เป็นตัวแปร $x$ กับ $y$ ซึ่งเป็นอินพุตและเอาต์พุตตามลำดับ ดังสมการต่อไปนี้

\begin{equation}
    y = mx + b
\end{equation}

\noindent โดยที่ $m$ คือความชันหรือน้ำหนัก (Weight) และ $b$ คือจุดตัดแกน $y$ หรือความโน้มเอียง (Bias) นั่นเอง สำหรับ Loss 
Function ที่เราจะเลือกใช้นั้น คือ Mean Square Error (MSE)

\begin{equation}
    \text{MSE} = \frac{1}{N} \sum_{i=1}^{n} (y_i - (m x_i + b))^2
\end{equation}

ในการ Optimize ฟังก์ชัน MSE ด้านบนนั้นเราจะใช้เทคนิค Gradient Descent ซึ่งเป็นเทคนิคที่เราจะคำนวณหา Gradient ซึ่งสามารถใช้สมการที่
\ref{eq:grad_simple_reg} ในการคำนวณได้

\begin{align}\label{eq:grad_simple_reg}
    f'(m,b) =
      \begin{bmatrix}
        \dv{f}{m}\\
        \dv{f}{b}\\
      \end{bmatrix}
    &=
      \begin{bmatrix}
        \frac{1}{N} \sum -x_i \cdot 2(y_i - (mx_i + b)) \\
        \frac{1}{N} \sum -1 \cdot 2(y_i - (mx_i + b)) \\
      \end{bmatrix}\\
    &=
      \begin{bmatrix}
         \frac{1}{N} \sum -2x_i(y_i - (mx_i + b)) \\
         \frac{1}{N} \sum -2(y_i - (mx_i + b)) \\
      \end{bmatrix}
\end{align}

หลังจากนั้นเราจะทำการฝึกสอนโมเดลโดยการใช้วิธีวนซ้ำเพื่อปรับค่าพารามิเตอร์ต่าง ๆ ทั้ง Weight และ Bias โดยภาพด้านล่างแสดงการเปลี่ยนแปลง%
ของการทาบเส้นตรงกับข้อมูล (Fitting) ระหว่างการฝึกสอน เราจะพบว่าเส้นตรง (Linear Line) ของเรานั้นจะลากผ่านข้อมูลที่อยู่ในช่วงบริเวณ%
ตรงกลางได้ดีขึ้นเรื่อย ๆ 

\begin{figure}[H]
    \centering
    \subfloat[ครั้งที่ 1\label{fig:plot_simple_reg_1}]{\includegraphics[width=0.4\linewidth]{fig/plot_simple_reg_1.png}}
    \hspace{1em}
    \subfloat[ครั้งที่ 2\label{fig:plot_simple_reg_2}]{\includegraphics[width=0.4\linewidth]{fig/plot_simple_reg_2.png}}
    \\
    \vspace{1em}
    \subfloat[ครั้งที่ 3\label{fig:plot_simple_reg_3}]{\includegraphics[width=0.4\linewidth]{fig/plot_simple_reg_3.png}}
    \hspace{1em}
    \subfloat[ครั้งที่ 4\label{fig:plot_simple_reg_4}]{\includegraphics[width=0.4\linewidth]{fig/plot_simple_reg_4.png}}
    \caption{การเปลี่ยนแปลงของเส้นตรงที่ถูกทาบ (Fitting) เข้ากับชุดข้อมูลอย่างง่าย}
    \label{fig:simple_reg_change}
\end{figure}

%--------------------------
\subsection{การถดถอยแบบหลายตัวแปร}
\label{sec:multi_lin_res}
\idxen{Supervised Learning!Multivariate Regression}
%--------------------------

สำหรับกรณีที่เรามีอินพุตหรือ Feature มากกว่าหนึ่งตัว เช่น ข้อมูลในตาราง \ref{tab:multi_reg_data} ด้านล่างที่เป็นการนำข้อมูลในตาราง 
\ref{tab:simple_reg_data} มาเพิ่มข้อมูลเงินที่ใช้ในการลงทุนสำหรับการโฆษณาทางสื่อโทรทัศน์และหนังสือพิมพ์เข้าไป (คอลัมน์ที่ 3 กับ 4) 

\begin{table}[H]
    \centering
    \caption{แสดงเงินที่ใช้ในการลงทุนการโฆษณาของบริษัทต่าง ๆ กับยอดขายรายปี}
    \label{tab:multi_reg_data}
    \small
    \begin{tabular}{lcccc}\toprule
    \textbf{บริษัท} &\textbf{วิทยุ} &\textbf{โทรทัศน์} &\textbf{หนังสือพิมพ์} &\textbf{ยอดขาย (ต่อหน่วย)} \\\midrule
    Amazon &37.8 &230.1 &69.1 &22.1 \\
    Google &39.3 &44.5 &23.1 &10.4 \\
    Facebook &45.9 &17.2 &34.7 &18.3 \\
    Apple &41.3 &151.5 &13.2 &18.5 \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/plot_multivar_reg.png}
    \caption{ความสัมพันธ์ของข้อมูลหลายตัวแปร (Multivariables Data)}
    \label{fig:multi_var_reg}
\end{figure}

โดยในกรณีที่ข้อมูลมีความซับซ้อนมากขึ้นแบบนี้ เราไม่สามารถใช้สมการเส้นตรงแบบง่าย ๆ ที่เราใช้ไปก่อนหน้านี้มาอธิบายความสัมพันธ์ระหว่าง Feature
ได้ ดังนั้นเราจะต้องมีการกำหนด Loss Function ขึ้นมาใหม่ โดยตอนนี้เราจะต้องมีการกำหนดค่า Weight ขึ้นมา 3 ค่า นั่นคือจากที่เราเคยมีฟังก์ชัน 
$mx + b$ ก็จะหลายเป็นฟังก์ชัน $W_1 x_1 + W_2 x_2 + W_3 x_3$ โดยสมการ Loss Function ใหม่ของเราจะมีหน้าตาดังนี้

\begin{equation}
    \text{MSE} = \frac{1}{2N} \sum_{i=1}^{n} (y_i - (W_1 x_1 + W_2 x_2 + W_3 x_3))^2
\end{equation}

สำหรับสมการที่เราจะมาใช้ในการหา Gradient ของกรณีนี้สามารถพิสูจน์ได้โดยใช้กฎลูกโซ่ (Chain Rule) เช่นเดียวกับกรณีก่อนหน้านี้

\begin{align}
    f'(W_1) = -x_1(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\
    f'(W_2) = -x_2(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\
    f'(W_3) = -x_3(y - (W_1 x_1 + W_2 x_2 + W_3 x_3))
\end{align}

%--------------------------
\section{การจำแนกประเภท}
\label{sec:classification}
\idxen{Supervised Learning!Classification}
%--------------------------

ในหัวข้อนี้จะเป็นการศึกษาโจทย์ปัญหาแบบการจำแนกประเภท (Classification) ซึ่งก็คล้าย ๆ กับโจทย์แบบ Regression แต่ว่าจะต่างกันตรงที่ค่า
$y$ ที่เราต้องการทำนายนั้นจะมีความไม่ต่อเนื่อง (Discrete Data) ซึ่งจะตรงข้ามกับ Regression ที่ค่า $y$ จะมีความต่อเนื่อง (Continuous Data) 
โดยเริ่มต้นเราจะสนใจกรณี Classification แบบง่ายก่อน นั่นก็คือมีประเภทของข้อมูลที่เราจะจำแนกเพียงแค่ 2 ประเภท เรียกว่าโจทย์ปัญหา 
Binary Classification ซึ่งค่า $y$ จะมีค่าได้แค่ 0 กับ 1 เท่านั้น ซึ่งในภายหลังเราจึงค่อยมาพิจารณากรณีที่มีประเภทมากกว่า 2 ประเภท
(Multiple-class Case) 

สำหรับการระบุชื่อของประเภทหรือคลาส (Class) นั้น เราจะเรียกคลาส 0 ว่าเป็น Negative Class และเรียกคลาส 1 ว่า Positive Class
ซึ่งบ่อยครั้งเรามักจะเจอการใช้เครื่องหมาย - และ + แทนการเขียน 0 กับ 1 โดยที่เราจะกำหนดให้ $y^{i}$ คือ Label สำหรับตัวอย่างการฝึกสอน

%--------------------------
\section{การถดถอยแบบโลจิสติค}
\label{sec:logis_regress}
\idxen{Supervised Learning!Logistic Regression}
%--------------------------

การวิเคราะห์การถดถอยโลจิสติค (Logistic Regression) เป็นการวิเคราะห์ที่มีเป้าหมายเพื่อทำนายโอกาสที่จะเกิดเหตุการณ์ที่สนใจโดยอาศัย%
สมการโลจิสติคที่สร้างขึ้นจากชุดตัวแปรทำนายที่เป็นตัวแปรที่มีข้อมูลอยู่ในระดับช่วงเป็นอย่างน้อย โดยที่ระหว่างตัวแปรทำนายจะต้องมีความสัมพันธ์กันต่ำ 
และในการวิเคราะห์จะต้องใช้ขนาดตัวแปรทำนายไม่ต่ำกว่า 30 ตัวแปร

%--------------------------
\section{เทคนิคอื่น ๆ}
\idxen{Machine Learning Techniques}
%--------------------------

%--------------------------
\subsection{Partial Least Squares (PLS)}
\label{sec:pls}
\idxen{Machine Learning Techniques!Partial Least Squares}
%--------------------------

วิธีกำลังสองน้อยที่สุดบางส่วน (Partial Least Squares หรือ PLS) เป็นวิธีเชิงสถิติที่ใช้สำหรับการวิเคราะห์หลายตัวแปรเพื่อสร้างตัวแบบ%
ความสัมพันธ์ระหว่างกลุ่มของตัวแปรทำนาย (Predictor Variable) โดยอาศัยตัวแปรแฝง (Latent variable) ซึ่งเทคนิคนี้มีความคล้ายกับ 
Principle Component Analysis (PCA) ซึ่งจะเป็นการลดจำนวนมิติของข้อมูล\autocite{wold1984} ในช่วงยุคเริ่มต้นที่มีการใช้ปัญญาประดิษฐ์%
ในงานด้านเคมีนั้น เทคนิคนี้ได้ถูกนำมาใช้อย่างแพร่หลาย เช่น นำมาใช้สำหรับการระบุ Vibrational Bands สำหรับ Vibrational Spectra 
และนำผลที่ได้มาเปรียบเทียบกับค่าการทำนายที่ได้จากวิธีอื่น เช่น ANN และ PCA-ANN

%--------------------------
\subsection{Gaussian Process Regression (GPR)}
\label{sec:gpr}
\idxen{Machine Learning Techniques!Gaussian Process Regression}
%--------------------------

การถดถอยของกระบวนการเกาส์เซียน (Gaussian Process Regression หรือ GPR) เป็นวิธีการถดถอยของเบส์แบบหนึ่งโดยใช้ Kernel Function 
ที่สามารถบ่งบอกหรือแสดงค่าความแปรปรวน (Covariance) ในขั้นตอน Gaussian Process ได้\autocite{rasmussen2005} โดย GPR 
จะทำการสร้างโมเดลแบบ Non-parametric และสามารถคำนวณค่าความเชื่อมั่น (Confidence intervals) ไปพร้อม ๆ กับการทำนาย 
รายละเอียดเพิ่มเติมของ GPR สามารถศึกษาได้ในหัวข้อ\ref{sec:gaussian_process} ของบทที่ \ref{ch:kernel}

%--------------------------
\subsection{Support Vector Machine (SVM)}
\label{sec:svm}
\idxen{Machine Learning Techniques!Support Vector Machine}
%--------------------------

Support Vector Machine (SVM) เป็นวิธีเคอร์เนลแบบหนึ่งที่มีความคล้ายกับ GPR หรือ KRR เป็นอย่างมาก โดย SVM จะทำการทำนายค่าโดย%
ทำการเปรียบเทียบข้อมูลใหม่กับข้อมูลอ้างอิง โดย SVM จะใช้ฟังก์ชัน $k(x_{i},x_{j})$ ในการเปรียบเทียบ (Comparison) 
และคำนวณค่าความเหมือน (Similarity) ระหว่างจุดสองจุด ซึ่งเราเรียกสิ่งนี้ว่าเคอร์เนล (Kernel) โดยความซับซ้อนของวิธีนี้นั้นไม่มีกฎเกณฑ์%
ที่แน่นอนในการกำหนด (Arbitrarily) ดังนั้นเราจะต้องทำการปรับ Hyperparameters เพื่อให้มีความเหมาะสมและสามารถควบคุมความซับซ้อนของวิธี 
SVM ซึ่งเราเรียกวิธีการปรับนี้ว่า Regularization เพื่อทำการหลีกเลี่ยงปัญหา Overfit นั่นเอง

%--------------------------
\subsection{Random Forest}
\label{sec:rs}
\idxen{Machine Learning Techniques!Random Forest}
%--------------------------

Random Forest (RF) เป็นวิธีหนึ่งในกลุ่มของโมเดลที่เรียกว่า Ensemble Learning (การเรียนรู้แบบกลุ่มก้อน) 
ที่มีหลักการคือการเทรนโมเดลที่เหมือนกันหลาย ๆ ครั้ง (Multitude) บนข้อมูลชุดเดียวกัน โดยแต่ละครั้งของการเทรนจะเลือกส่วนของข้อมูลที่%
เทรนไม่เหมือนกัน แล้วนำการตัดสินใจของโมเดลเหล่านั้นมาโหวตกันว่า Class ไหนถูกเลือกมากที่สุด\autocite{breiman2001,quinlan1986}

%--------------------------
\subsection{Artificial Neural Network}
\label{sec:ann}
\idxen{Machine Learning Techniques!Artificial Neural Network}
%--------------------------

โครงข่ายประสาทเทียมประดิษฐ์ (Artificial Neural Network หรือ ANN) หรือเรียกว่าโครงข่ายประสาทเทียม (Neural Network หรือ 
Neural Net) เป็นอัลกอริทึมรูปแบบหนึ่งที่เลียนแบบการทำงานของสมองมนุษย์ โดยทำการสร้างโมเดลเรียนรู้ที่ประกอบไปด้วยชั้นเรียนรู้ระหว่างกลาง 
(Hidden Layer) และหน่วยย่อยที่เกิดการเรียนรู้ (Node หรือ Artificial Neuron หรือ Unit)

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/neuron.png}
    \caption{การรับส่งข้อมูลภายในเซลล์ประสาท}
    \label{fig:neuron}
\end{figure}

จริง ๆ แล้ว ANN ก็คือการจำลองสมองมนุษย์โดยพยายามสร้างองค์ประกอบต่าง ๆ ให้มีความคล้ายกันให้มากที่สุด เช่น ในมืองมีเซลล์ประสาท (Neurons) 
และ จุดประสานประสาท (Synapses) แต่ละเซลล์ประสาทประกอบด้วยปลายในการรับกระแสประสาทเรียกว่า \enquote{เดนไดรท์} (Dendrite) 
ซึ่งเป็นอินพุต (Input) และปลายในการส่งกระแสประสาทเรียกว่า Axon ซึ่งเปรียบเหมือนเป็นเอาต์พุต (Output) ของเซลล์

โดยโมเดลโครงข่ายประสาทเทียมที่มีการนำไปใช้มากที่สุดคือเครือข่ายประสาทแบบป้อนไปหน้า (Feedforward Network) และโมเดล ANN 
ยังสามารถแบ่งออกได้เป็นหลายประเภท ดังนี้

\begin{itemize}
    \item เพอร์เซ็ปตรอนชั้นเดียว (Single-layer Perceptron)
    \item เพอร์เซ็ปตรอนหลายชั้น (Multi-layer Perceptron
    \item โครงข่ายแบบวนซ้ำ (Recurrent Network)
    \item แผนผังจัดระเบียบเองได้ (Self-organizing Map)
    \item เครื่องจักรโบลทซ์แมน (Boltzmann Machine)
    \item กลไกแบบคณะกรรมการ (Committee of Machines)
    \item โครงข่ายความสัมพันธ์ (Associative Neural Network)
    \item โครงข่ายกึ่งสำเร็จรูป (Instantaneously Trained Networks)
    \item โครงข่ายแบบยิงกระตุ้น (Spiking Neural Networks) 
\end{itemize}

% LaTeX source for ``การเรียนรู้ของเครื่องสำหรับเคมีควอนตัม (Machine Learning for Quantum Chemistry)''
% Copyright (c) 2022 รังสิมันต์ เกษแก้ว (Rangsiman Ketkaew).

% License: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)
% https://creativecommons.org/licenses/by-nc-nd/4.0/

\chapter{วิธีเคอร์เนล}
\label{ch:kernel}

%--------------------------
\section{เคอร์แนลคืออะไร}
%--------------------------

ในบทที่แล้วเราได้เรียนรู้เทคนิค Linear Regression (การถดถอยแบบเส้นตรง) กันไปแล้ว ซึ่งเป็นกรณีที่เราเจอปัญหาที่เกี่ยวข้องกับความสัมพันธ์%
ของตัวแปรสองตัว ซึ่งเราสามารถทำการ Fit สมการเชิงเส้นของตัวแปร X ให้เข้ากับชุดข้อมูล (Training Data) แล้วถ้าหากค่า Y หรือ Output 
ทีี่เราต้องการทำนายนั้นสามารถถูกทำนายหรืออธิบายได้ดีกว่าด้วยสมการไม่เชิงเส้น (Non-linear Function) ของตัวแปร X นั้น เราจะต้องใช้สิ่ง%
ที่เรียกว่าเคอร์เนล (Kernel) ซึ่งเราจะมาเรียนรู้กันในบทนี้ แต่ก่อนที่จะอธิบาย Kernel นั้น เราควรจะมาเข้าใจสิ่งที่เป็นพื้นฐานกันก่อนนั่นก็คือ Feature Map ซึ่งเป็นสิ่งที่ทำการเชื่อมโยง Atrribute ให้เข้ากับ 
Feature ซึ่งเราเรียกกระบวนการนี้ว่า Mapping 

เริ่มต้นด้วยการพิจารณาการ Fit ฟังก์ชันแบบ Cubic Function ซึ่งมีหน้าตาสมการดังนี้

\begin{equation}\label{eq:cubic_func}
    y = \theta_{3}x^{3} + \theta_{2}x^{2} + \theta_{1}x^{1} + \theta_{0}
\end{equation}

จะเห็นได้ว่าเราสามารถที่จะมอง Cubic Function ด้านบนเป็นสมการเชิงเส้นง่าย ๆ ซึ่งสมการ \ref{eq:cubic_func} นั้นจะขึ้นอยู่กับ Feature 
Variables (x) ที่เรากำหนดไว้นั่นเอง คราวนี้เราลองมากำหนดฟังก์ชันใหม่โดยอ้างอิงสมการเดิม ซึ่งเราจะมีการกำหนดเซตของตัวแปร x อันใหม่ขึ้นมานั่นคือ

\begin{align}\label{eq:cubic_func_2}
    y &= \theta_{3}x^{3} + \theta_{2}x^{2} + \theta_{1}x^{1} + \theta_{0} \nonumber \\ 
      &= \theta^{\top}\phi(x)
\end{align}

โดยที่ $\phi$ คือเวกเตอร์ของตัวแปร Input ดังนี้

\begin{equation}
\phi = 
\begin{bmatrix}
    1 \\
    x \\
    x^{2} \\
    x^{3} 
\end{bmatrix}
\end{equation}

และ $\theta^{\top}$ เป็นเวกเตอร์ของพารามิเตอร์ $\theta_{i}$ ดังนี้

\begin{equation}
\theta^{\top} =
\begin{bmatrix}
    \theta_{o} & \theta_{1} & \theta_{2} & \theta_{3}
\end{bmatrix}
\end{equation}
 
ซึ่งคำถามที่ตามมาก็คือเราจะแยกความแตกต่างระหว่างสมการ \ref{eq:cubic_func} กับ \ref{eq:cubic_func_2} ได้อย่างไร เราสามารถ%
ทำได้โดยการกำหนดให้ตัวแปร Input x ของเรานั้นเป็น Attribute เมื่อเราทำการ Map หรือเชื่อมโยงตัวแปร x ของเราไปยังปริมาณตัวใหม่ที่เป็น
$\phi(x)$ เราจะเรียกปริมาณตัวนี้ว่า Feature และฟังก์ชันที่เราใช้ในการ Mapping นั้นเราเรียกว่า Feature Map ($\phi$) ซึ่งเป็นตัวที่ทำการ5
เชื่อมโยงความสัมพันธ์ของ Attribute ไปยัง Feature

ดังนั้นโจทย์ของเรานั้นก็คือเราจะต้องมาทำการหาอัลกอริทึม Gradient Descent ที่จะนำมาใช้ในการ Fitting โมเดลของเรา 
($\theta^{\top}\phi(x)$ ก็คือโมเดลของเรานั่นเอง) ซึ่งสามารถทำได้โดยใช้ Stocahstic Gradient Descent โดยสมการมีหน้าตาดังต่อไปนี้

\begin{equation}\label{eq:sto_grad_des}
    \theta := \theta + \alpha (y^{i} - \theta^{\top}\phi(x^{i}))\phi(x^{i})
\end{equation}

โดยที่ $\alpha$ คือขนาดของการก้าวเดิน (Step Size) หรืออัตราการเรียนรู้ (Learning Rate) ซึ่งจะเป็นพารามิเตอร์ที่จะปรับความเร็วในการ%
Optimize เกรเดียนต์ (สำหรับรายละเอียดเพิ่มเติมเกี่ยวกับการพิสูจน์สมการที่ \ref{eq:sto_grad_des} ผู้อ่านสามารถอ่านได้จากหนังสือปัญญาประดิษฐ์ทั่วไป) 
แต่ทว่าปัญหาอันหนึ่งของ Stochastic Gradient Descent นั้นก็คือมีไม่สามารถที่จะหาผลเฉลยได้ง่าย ๆ ซึ่งทำให้มีความสิ้นเปลืองในการคำนวณเป็นอย่างมาก 
(Computationally Expensive) โดยเฉพาะอย่างยิ่งเมื่อ Feature ของเรา ($\phi(x)$) นั้นมีจำนวนมิติที่เยอะมาก ๆ

สำหรับนิยามของเคอร์เนล ($K$) นั้นจะเกี่ยวข้องกับการหาความเชื่อมโยงระหว่างตัวแปรของตัว ซึ่งความเชื่อมโยงในที่นี้ก็คือความเหมือน (Similarity)
ระหว่างตัวแปรนั่นเอง เรามีการกำหนดเคอร์เนลให้อยู่ในรูปของ Feature Map ($\phi$) ซึ่งเป็นฟังก์ชันที่ทำการ Mapping ปริภูมิของตัวแปร Input x
ที่ได้อธิบายไว้ก่อนหน้านี้ ($\chi \times \chi \rightarrow \mathbb{R}$) ดังนี้

\begin{equation}
    K(x,z) = \langle\phi(x),\phi(z)\rangle
\end{equation}

\noindent ซึ่งเราสามารถคำนวณ $\langle\phi(x),\phi(z)\rangle$ ได้โดยการใช้สมการต่อไปนี้

\begin{align}
    \langle\phi(x),\phi(z)\rangle =& 1 + \sum_{i=1}^d x_i z_i + \sum_{i,j\in\{1,\ldots,d\}} x_i x_j z_i z_j \\
    &+ \sum_{i,j,k \in \{1,\ldots,d\}} x_i x_j x_k z_i z_j z_k \nonumber \\
    =& 1 + \sum_{i=1}^d x_i z_i + \left(\sum_{i=1}^d x_i z_i \right)^2 + \left( \sum_{i=1}^d x_i z_i \right)^3 \\
    =& 1 + \langle x,z \rangle + \langle x,z \rangle^2 + \langle x,z \rangle^3\label{eq:feat_map_inner_product}
\end{align}

\noindent อธิบายแบบง่าย ๆ ก็คือเราจะคำนวณพจน์แรก $ \langle x,z \rangle$ ของสมการ \ref{eq:feat_map_inner_product} ก่อน 
หลังจากนั้นจึงคำนวณพจน์อื่น ๆ ที่เหลือ (กำหนดให้ $i,j$ เป็นสมาชิกของเซต \{1, \dots, n\})

%--------------------------
\section{ฟังก์ชันเคอร์เนลและคุณสมบัติของเคอร์แนล}
%--------------------------

ในหัวข้อนี้เราจะมาดูรายละเอียดของเคอร์เนลกันว่า $K(x,z)$ มีคุณสมบัติอะไรที่น่าสนใจบ้าง สำหรับสัญลักษณ์ที่เราจะกำหนดขึ้นมาเพื่ออธิบายเคอร์เนลนั้น5
จะเป็น $K(\cdot,\cdot)$ หรือเรียกง่าย ๆ ว่าเป็นฟังก์ชันเคอร์เนล (Kernel Function) ก็ได้ 

เริ่มต้นด้วยการยกตัวอย่างฟังก์ชันเคอร์เนลแบบเรียบง่าย เช่น

\begin{equation}
    K(x,z) = (x^{\top} z)^{2}
\end{equation}

\noindent ซึ่งสามารถเขียนได้เป็น

\begin{align}
    K(x,z) &= \left( \sum_{i=1}^d x_i z_i \right) \left( \sum_{j=1}^d x_j z_j \right)\\
    &= \sum_{i=1}^d \sum_{j=1}^d x_i x_j z_i z_j\\
    &= \sum_{i,j=1}^d (x_i x_j)(z_i z_j)
\end{align}

\noindent ดังนั้น จะเห็นได้ชัดเลยว่าจริง ๆ แล้วนั้น $K(x,z) = \langle\phi(x),\phi(z)\rangle$ เป็นฟังก์ชันเคอร์เนลที่สอดคล้องกับ
Feature Mapping ($\phi$) โดยที่มีสมการเป็น (กรณีที่ $d = 3$)

\begin{equation}
    \phi(x) = \begin{bmatrix}
    x_1 x_1\\
    x_1 x_2\\
    x_1 x_3\\
    x_2 x_1\\
    x_2 x_2\\
    x_2 x_3\\
    x_3 x_1\\
    x_3 x_2\\
    x_3 x_3\\
    \end{bmatrix}
\end{equation}

เราลองมาดูตัวอย่างที่สองของ $K(\cdot,\cdot)$ ซึ่งถูกกำหนดด้วยฟังก์ชันเชิงเส้นดังต่อไปนี้

\begin{align}
    K(x,z) &= (x^{\top} z + c)^2\\
    &= \sum_{i,j=1}^d (x_i x_j)(z_i z_j) + \sum_{i=1}^d \left(\sqrt{2c}x_i\right) \left(\sqrt{2c}z_i\right) + c^2.
\end{align}

\noindent โดยที่ฟังก์ชันเคอร์เนลด้านบนนี้ก็จะคล้าย ๆ กับก่อนหน้านี้แต่จะมีความแตกต่างตรงที่มีการเพิ่มพารามิเตอร์ $c$ เข้ามา ซึ่งเป็นตัวที่กำหนด%
การถ่วงน้ำหนัก (Weighting) ระหว่าง $x_{i}$ และ $x_{i}x_{j}$ โดยที่เรามองได้ง่าย ๆ ก็คือเคอร์เนล $K(x,z) = (x^{\top} z + c)^2$
นั้นจะมีความสอดคล้องกับ Fature Mapping ไปยังปริภูมิของ $\binom{d+k}{k}$

คราวนี้เราลองมามองเคอร์เนลให้เป็นเมตริกหรือตัววัดความเหมือนกันระหว่าง Feature Mapping (Similarity Metrics) เราเริ่มต้นด้วยสมมติฐาน%
ว่าถ้ากรณีที่ $\phi(x)$ กับ $\phi(z)$ บนปริภูมินั้นมีความใกล้กันมาก ๆ เราอาจจะคาดการณ์ได้ว่า $K(x,z) = \phi(x)^{\top} \phi(z$
จะมีขนาดที่ใหญ่มากเพราะว่ามีการซ้อนทับกันเยอะ (เรานิยามให้การซ้อนทับกันหรือ Overlap นั้นเป็นความเหมือนหรือ Similarity) ในกรณีที่ตรงข้ามกัน
ถ้าหาก $\phi(x)$ กับ $\phi(z)$ อยู่ห่างกันมาก จะทำให้ Overlap นั้นมีน้อยมาก จึงทำขนาดของ $K(x,z) = \phi(x)^{\top} \phi(z$ 
มีขนาดที่เล็กตามไปด้วย ซึ่งการที่เราสามารถนิยามเคอร์เนลให้เป็นมาตรวัดความเหมือนหรือความแตกต่างระหว่าง $x$ และ $z$ นั้นมีประโยชน์อย่างมาก%
นำไปแก้ปัญหาหลาย ๆ อย่าง แต่ว่าฟังก์ชันที่เราเลือกมาใช้ในการอธิบายความแตกต่างของทั้งสองตัวแปรนั้นจะต้องมีความสมเหตุสมผล โดยฟังก์ชันที่อาจจะ%
เรียกได้ว่าได้รับความนิยมในการนำมาใช้เป็นฟังก์ชันเคอร์เนลนั้นก็คือฟังก์ชันเกาส์เซียน (Gaussian Function) ซึ่งมีสมการดังต่อไปนี้ 

\begin{equation}
    K(x,z) = \exp\left(-\frac{\lVert x - z \rVert^2}{2\sigma^2}\right).    
\end{equation}

\noindent ฟังก์ชันด้านบนนั้นเมื่อถูกนำมาใช้เป็นเคอร์เนลแล้ว เราจะเรียกเคอร์เนลนี้ว่า Gaussian Kernel ซึ่งเป็นฟังก์ชันที่เหมาะสมมาก ๆ เพราะ%
ว่ามีความสมมาตร มีความต่อเนื่องตลอดช่วงของปริภูมิ และมีค่าเข้าใกล้ 1 เมื่อ $x$ และ $z$ นั้นอยู่ใกล้กัน และมีค่าเข้าใกล้ 0 เมื่อ $x$ และ $z$ 
อยู่ห่างกัน

%--------------------------
\subsection{Linear Regression}
%--------------------------

%--------------------------
\subsection{Ridge Regression}
%--------------------------

%--------------------------
\section{Kernel Ridge Regression}
%--------------------------

Kernel Ridge Regression (KRR) เป็นการต่อยอดจาก Ridge Regression หรืออธิบายง่าย ๆ ว่า KRR คือ RR ในเวอร์ชันที่เป็น Nonlinear problem

%--------------------------
\section{Gaussian Process Regression}
%--------------------------

%--------------------------
\section{Support Vector Machine}
%--------------------------

